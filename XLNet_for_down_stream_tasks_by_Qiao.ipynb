{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Huggingface_transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjB68KGLb26k",
        "colab_type": "text"
      },
      "source": [
        "## 利用[Huggingface](https://huggingface.co/transformers/installation.html#)实现的预训练语言模型做下游任务\n",
        "by Qiao for NLP7 2020-8-16\n",
        "\n",
        "预训练语言模型的用法：\n",
        "1. 作为特征提取器\n",
        "2. 作为encoder参与下游任务微调\n",
        "使用上非常类似，差别是后者在训练过程中原预训练语言模型的参数也允许优化。\n",
        "\n",
        "主要内容:\n",
        "1. 以XLNet介绍HuggingFace transformers组件的使用套路\n",
        "2. 以XLNet为例介绍如何接续下游的文本分类和抽取式问答。\n",
        "\n",
        "主要参考[文档](https://huggingface.co/transformers/model_doc/xlnet.html)和[代码](https://github.com/huggingface/transformers/blob/0ed7c00ba6b3178c8c323a0440bf1221fb99784b/src/transformers/modeling_xlnet.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dw21tvyb26k",
        "colab_type": "text"
      },
      "source": [
        "### 以[XLNet](https://huggingface.co/transformers/model_doc/xlnet.html)为例，使用其他Huggingface封装的预训练语言模型的套路与类似"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD69bu-ib26l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "1deab98c-f515-49c3-fd25-61ce6dda7826"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "!pip install transformers\n",
        "from transformers import XLNetModel, XLNetTokenizer, XLNetConfig"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hf6yUBab26n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "model = XLNetModel.from_pretrained('xlnet-base-cased', \n",
        "                                   output_hidden_states=True,\n",
        "                                   output_attentions=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA4TQ7Xnb26p",
        "colab_type": "text"
      },
      "source": [
        "### 注意：\n",
        "以上使用模型名称初始化的模块，程序会在后台下载预训练完成的XLNet模型并加载。对于内地同学，除改变上网方式外，还可以手动下载模型，指定路径加载。\n",
        "#### 手动下载模型：\n",
        "在HuggingFace官方[模型库](https://huggingface.co/models)上找到需要下载的模型，点击模型链接，例如：[xlnet-base-cased](https://huggingface.co/xlnet-base-cased)模型。在跳转到的模型页面中点击`List all files in model`（字比较小，注意查看），将跳出框中的模型相关文(pytorch或tf版本)件保存到本地。\n",
        "![image.png](https://raw.githubusercontent.com/qiaochen/NLPLecturePreparation/master/XLNetModel.PNG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSoaGwNlb26q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # 本地加载XLNet模型\n",
        "# MODEL_PATH = r\"D:\\data\\nlp\\xlnet-model/\"\n",
        "# config = XLNetConfig.from_json_file(os.path.join(MODEL_PATH, \"xlnet-base-cased-config.json\"))\n",
        "\n",
        "# #config文件不仅用于设置模型参数，也可以用来控制模型的行为\n",
        "# config.output_hidden_states = True\n",
        "# config.output_attentions = True\n",
        "\n",
        "# tokenizer = XLNetTokenizer(os.path.join(MODEL_PATH, 'xlnet-base-cased-spiece.model'))\n",
        "# model = XLNetModel.from_pretrained(MODEL_PATH, config = config)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwIhUM-gb26r",
        "colab_type": "text"
      },
      "source": [
        "### 1. 句子到token id转换"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF5mo_D5b26s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "e6fdaf5b-0245-4231-ce04-a92699d30963"
      },
      "source": [
        "# 利用tokenizer将原始的句子准备成模型输入\n",
        "sentence = \"This is an interesting review session\"\n",
        "\n",
        "# tokenization\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(\"Tokens: {}\".format(tokens))\n",
        "\n",
        "# 将token转化为ID\n",
        "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Tokens id: {}\".format(tokens_ids))\n",
        "\n",
        "# 添加特殊token: <cls>, <sep>\n",
        "tokens_ids = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
        "\n",
        "# 准备成pytorch tensor\n",
        "tokens_pt = torch.tensor([tokens_ids])\n",
        "print(\"Tokens PyTorch: {}\".format(tokens_pt))\n",
        "\n",
        "# print(tokenizer.convert_ids_to_tokens([122,   27,   48, 5272,  717,    4,    3]))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens: ['▁This', '▁is', '▁an', '▁interesting', '▁review', '▁session']\n",
            "Tokens id: [122, 27, 48, 2456, 1398, 1961]\n",
            "Tokens PyTorch: tensor([[ 122,   27,   48, 2456, 1398, 1961,    4,    3]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqhXQD9pb26u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "923e13eb-a3f7-42f7-d067-46eb400154aa"
      },
      "source": [
        "# 偷懒的一条龙服务\n",
        "tokens_pt2 = tokenizer(sentence, return_tensors=\"pt\")\n",
        "print(\"Tokens PyTorch: {}\".format(tokens_pt2))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens PyTorch: {'input_ids': tensor([[ 122,   27,   48, 2456, 1398, 1961,    4,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rN4h6CFb26w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "f4d6b1e6-a86c-4c98-bb5d-4ed01720201d"
      },
      "source": [
        "# 批处理\n",
        "# padding\n",
        "sentences = [\"The ultimate answer to life, universe and time is 42.\", \"Take a towel for a space travel.\"]\n",
        "print(\"Batch tokenization:\\n\", tokenizer(sentences)['input_ids'])\n",
        "print(\"With Padding:\\n\", tokenizer(sentences, padding=True)['input_ids'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch tokenization:\n",
            " [[32, 6452, 1543, 22, 235, 19, 6486, 21, 92, 27, 4087, 9, 4, 3], [3636, 24, 14680, 28, 24, 888, 1316, 9, 4, 3]]\n",
            "With Padding:\n",
            " [[32, 6452, 1543, 22, 235, 19, 6486, 21, 92, 27, 4087, 9, 4, 3], [5, 5, 5, 5, 3636, 24, 14680, 28, 24, 888, 1316, 9, 4, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNv9Hw8Mb26y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "4d102f32-2a40-4b07-a28e-06153515d67f"
      },
      "source": [
        "# 输入句子对：\n",
        "multi_seg_input = tokenizer(\"This is segment A\", \"This is segment B\")\n",
        "print(\"Multi segment token (str): {}\".format(tokenizer.convert_ids_to_tokens(multi_seg_input['input_ids'])))\n",
        "print(\"Multi segment token (int): {}\".format(multi_seg_input['input_ids']))\n",
        "print(\"Multi segment type       : {}\".format(multi_seg_input['token_type_ids']))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multi segment token (str): ['▁This', '▁is', '▁segment', '▁A', '<sep>', '▁This', '▁is', '▁segment', '▁B', '<sep>', '<cls>']\n",
            "Multi segment token (int): [122, 27, 7295, 79, 4, 122, 27, 7295, 322, 4, 3]\n",
            "Multi segment type       : [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfjZ9f_wb260",
        "colab_type": "text"
      },
      "source": [
        "### 2. 模型encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxzgyb5jb260",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "51662acf-2755-44cc-f505-1fe1ac119383"
      },
      "source": [
        "# 默认情况下，model.dev()模式下。下面使用模型Encode输入的句子\n",
        "# 因为我们在config中设置模型返回每层的hidden states和注意力，再加上默认输出的最后一层隐状态，输出有3个部分\n",
        "print(\"Is training mode ? \", model.training)\n",
        "\n",
        "sentence = \"The ultimate answer to life, universe and time is 42.\"\n",
        "\n",
        "tokens_pt = tokenizer(sentence, return_tensors=\"pt\")\n",
        "print(\"Token (str): {}\".format(\n",
        "    tokenizer.convert_ids_to_tokens(tokens_pt['input_ids'][0])\n",
        "    ))\n",
        "\n",
        "final_layer_h, all_layer_h, attentions = model(**tokens_pt)\n",
        "\n",
        "print(torch.sum(final_layer_h - all_layer_h[-1]).item())\n",
        "\n",
        "final_layer_h.shape, len(all_layer_h), len(attentions)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is training mode ?  False\n",
            "Token (str): ['▁The', '▁ultimate', '▁answer', '▁to', '▁life', ',', '▁universe', '▁and', '▁time', '▁is', '▁42', '.', '<sep>', '<cls>']\n",
            "0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 14, 768]), 13, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbV9RUCcb262",
        "colab_type": "text"
      },
      "source": [
        "### 3. 下游任务\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDuKbswbb263",
        "colab_type": "text"
      },
      "source": [
        "### 例1. 文本分类"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Hl4iFib263",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class XLNetSeqSummary(nn.Module):\n",
        "    \n",
        "    def __init__(self, \n",
        "                 how='cls', \n",
        "                 hidden_size=768, \n",
        "                 activation=None, \n",
        "                 first_dropout=None, \n",
        "                 last_dropout=None):\n",
        "        super().__init__()\n",
        "        self.how = how\n",
        "        self.summary = nn.Linear(hidden_size, hidden_size)\n",
        "        self.activation = activation if activation else nn.GELU()\n",
        "        self.first_dropout = first_dropout if first_dropout else nn.Dropout(0.5)\n",
        "        self.last_dropout = last_dropout if last_dropout else nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        \"\"\"\n",
        "        对隐状态序列池化或返回cls处的表示，作为句子的encoding\n",
        "        Args:\n",
        "            hidden_states :\n",
        "                XLNet模型输出的最后层隐状态序列.\n",
        "        Returns:\n",
        "            : 句子向量表示\n",
        "        \"\"\"\n",
        "        if self.how == \"cls\":\n",
        "            output = hidden_states[:, -1]\n",
        "        elif self.how == \"mean\":\n",
        "            output = hidden_states.mean(dim=1)\n",
        "        elif self.how == \"max\":\n",
        "            output = hidden_states.max(dim=1)\n",
        "        else:\n",
        "            raise Exception(\"Summary type '{}' not implemted.\".format(self.how))\n",
        "\n",
        "        output = self.first_dropout(output)\n",
        "        output = self.summary(output)\n",
        "        output = self.activation(output)\n",
        "        output = self.last_dropout(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class XLNetSentenceClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 num_labels,\n",
        "                 xlnet_model,\n",
        "                 d_model=768):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.d_model = d_model\n",
        "        self.transformer = xlnet_model\n",
        "        self.sequence_summary = XLNetSeqSummary('cls', d_model, nn.GELU())\n",
        "        self.logits_proj = nn.Linear(d_model, num_labels)\n",
        "        \n",
        "    def forward(self, model_inputs):\n",
        "        transformer_outputs = self.transformer(**model_inputs)\n",
        "            \n",
        "        output = transformer_outputs[0]\n",
        "        output = self.sequence_summary(output)\n",
        "        logits = self.logits_proj(output)\n",
        "\n",
        "        return logits\n",
        "    \n",
        "def get_loss(criterion, logits, labels):\n",
        "    return criterion(logits, labels)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvLk2n2s11dm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f908050-366c-4067-864f-3fa7f2b1126e"
      },
      "source": [
        "# 验证forward和反向传播\n",
        "\n",
        "# toy examples\n",
        "sentences = [\"The ultimate answer to life, universe and time is 42.\", \n",
        "             \"Take a towel for a space travel.\"]\n",
        "labels = torch.LongTensor([0, 1])\n",
        "\n",
        "# 实例化各个模块\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "classifier = XLNetSentenceClassifier(2, model, 768)\n",
        "optimizer = torch.optim.AdamW(classifier.parameters())\n",
        "\n",
        "# forward + loss\n",
        "classifier.train()\n",
        "optimizer.zero_grad()\n",
        "logits = classifier(tokenizer(sentences, padding=True, return_tensors='pt'))\n",
        "loss = get_loss(criterion, logits, labels)\n",
        "\n",
        "print(\"Loss: \", loss.item())\n",
        "\n",
        "# backwawrd step\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"=\"*25)\n",
        "print(\"Confirm that the gradients are computed for the original XLNet parameters.\\n\")\n",
        "print(\"=\"*25)\n",
        "for param in classifier.parameters():\n",
        "    print(param.shape, param.grad.sum() if not param.grad is None else param.grad)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_xlnet.py:283: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
            "  attn_score = (ac + bd + ef) * self.scale\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  0.37429606914520264\n",
            "=========================\n",
            "Confirm that the gradients are computed for the original XLNet parameters.\n",
            "\n",
            "=========================\n",
            "torch.Size([1, 1, 768]) None\n",
            "torch.Size([32000, 768]) tensor(4.3674)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0620)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0048)\n",
            "torch.Size([768, 12, 64]) tensor(-1.9110)\n",
            "torch.Size([768, 12, 64]) tensor(1.8535)\n",
            "torch.Size([768, 12, 64]) tensor(1.4339)\n",
            "torch.Size([12, 64]) tensor(0.3109)\n",
            "torch.Size([12, 64]) tensor(-0.0349)\n",
            "torch.Size([12, 64]) tensor(0.0297)\n",
            "torch.Size([2, 12, 64]) tensor(-4.7185e-09)\n",
            "torch.Size([768]) tensor(-0.0299)\n",
            "torch.Size([768]) tensor(0.0196)\n",
            "torch.Size([768]) tensor(1.5420)\n",
            "torch.Size([768]) tensor(0.2207)\n",
            "torch.Size([3072, 768]) tensor(12.8594)\n",
            "torch.Size([3072]) tensor(-0.5156)\n",
            "torch.Size([768, 3072]) tensor(-3.1910)\n",
            "torch.Size([768]) tensor(0.1854)\n",
            "torch.Size([768, 12, 64]) tensor(7.0323)\n",
            "torch.Size([768, 12, 64]) tensor(0.7410)\n",
            "torch.Size([768, 12, 64]) tensor(-2.7717)\n",
            "torch.Size([768, 12, 64]) tensor(-1.5112)\n",
            "torch.Size([768, 12, 64]) tensor(4.3989)\n",
            "torch.Size([12, 64]) tensor(-0.0579)\n",
            "torch.Size([12, 64]) tensor(-0.0001)\n",
            "torch.Size([12, 64]) tensor(0.1017)\n",
            "torch.Size([2, 12, 64]) tensor(8.6082e-08)\n",
            "torch.Size([768]) tensor(-1.3530)\n",
            "torch.Size([768]) tensor(0.2900)\n",
            "torch.Size([768]) tensor(-0.4874)\n",
            "torch.Size([768]) tensor(0.8502)\n",
            "torch.Size([3072, 768]) tensor(-8.0809)\n",
            "torch.Size([3072]) tensor(-0.8348)\n",
            "torch.Size([768, 3072]) tensor(8.2589)\n",
            "torch.Size([768]) tensor(-0.3614)\n",
            "torch.Size([768, 12, 64]) tensor(-2.5109)\n",
            "torch.Size([768, 12, 64]) tensor(-0.3346)\n",
            "torch.Size([768, 12, 64]) tensor(-0.5476)\n",
            "torch.Size([768, 12, 64]) tensor(1.2904)\n",
            "torch.Size([768, 12, 64]) tensor(9.7423)\n",
            "torch.Size([12, 64]) tensor(-0.1084)\n",
            "torch.Size([12, 64]) tensor(0.0217)\n",
            "torch.Size([12, 64]) tensor(0.2224)\n",
            "torch.Size([2, 12, 64]) tensor(-4.7730e-09)\n",
            "torch.Size([768]) tensor(-0.1931)\n",
            "torch.Size([768]) tensor(0.0082)\n",
            "torch.Size([768]) tensor(1.3613)\n",
            "torch.Size([768]) tensor(2.7523)\n",
            "torch.Size([3072, 768]) tensor(0.1170)\n",
            "torch.Size([3072]) tensor(0.2254)\n",
            "torch.Size([768, 3072]) tensor(-11.7237)\n",
            "torch.Size([768]) tensor(0.0959)\n",
            "torch.Size([768, 12, 64]) tensor(-1.1274)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0647)\n",
            "torch.Size([768, 12, 64]) tensor(-5.8271)\n",
            "torch.Size([768, 12, 64]) tensor(0.5369)\n",
            "torch.Size([768, 12, 64]) tensor(-10.8905)\n",
            "torch.Size([12, 64]) tensor(-0.0177)\n",
            "torch.Size([12, 64]) tensor(-0.0100)\n",
            "torch.Size([12, 64]) tensor(-0.0283)\n",
            "torch.Size([2, 12, 64]) tensor(8.7311e-11)\n",
            "torch.Size([768]) tensor(0.8224)\n",
            "torch.Size([768]) tensor(-0.3138)\n",
            "torch.Size([768]) tensor(0.7709)\n",
            "torch.Size([768]) tensor(-3.7109)\n",
            "torch.Size([3072, 768]) tensor(-13.0309)\n",
            "torch.Size([3072]) tensor(0.6825)\n",
            "torch.Size([768, 3072]) tensor(11.9302)\n",
            "torch.Size([768]) tensor(-0.0476)\n",
            "torch.Size([768, 12, 64]) tensor(9.2524)\n",
            "torch.Size([768, 12, 64]) tensor(-1.5572)\n",
            "torch.Size([768, 12, 64]) tensor(-27.9219)\n",
            "torch.Size([768, 12, 64]) tensor(1.3677)\n",
            "torch.Size([768, 12, 64]) tensor(-14.2867)\n",
            "torch.Size([12, 64]) tensor(0.0303)\n",
            "torch.Size([12, 64]) tensor(0.0054)\n",
            "torch.Size([12, 64]) tensor(0.2244)\n",
            "torch.Size([2, 12, 64]) tensor(-2.0853e-08)\n",
            "torch.Size([768]) tensor(-0.1902)\n",
            "torch.Size([768]) tensor(-0.1266)\n",
            "torch.Size([768]) tensor(1.4741)\n",
            "torch.Size([768]) tensor(-3.8543)\n",
            "torch.Size([3072, 768]) tensor(-7.0477)\n",
            "torch.Size([3072]) tensor(0.2935)\n",
            "torch.Size([768, 3072]) tensor(32.9982)\n",
            "torch.Size([768]) tensor(-0.2564)\n",
            "torch.Size([768, 12, 64]) tensor(1.8549)\n",
            "torch.Size([768, 12, 64]) tensor(-0.1922)\n",
            "torch.Size([768, 12, 64]) tensor(-9.4280)\n",
            "torch.Size([768, 12, 64]) tensor(-0.6116)\n",
            "torch.Size([768, 12, 64]) tensor(0.3952)\n",
            "torch.Size([12, 64]) tensor(-0.1445)\n",
            "torch.Size([12, 64]) tensor(-0.0058)\n",
            "torch.Size([12, 64]) tensor(0.0095)\n",
            "torch.Size([2, 12, 64]) tensor(-4.1378e-08)\n",
            "torch.Size([768]) tensor(-0.0633)\n",
            "torch.Size([768]) tensor(0.4471)\n",
            "torch.Size([768]) tensor(0.5628)\n",
            "torch.Size([768]) tensor(4.8528)\n",
            "torch.Size([3072, 768]) tensor(5.0159)\n",
            "torch.Size([3072]) tensor(-0.2102)\n",
            "torch.Size([768, 3072]) tensor(-9.1904)\n",
            "torch.Size([768]) tensor(0.0882)\n",
            "torch.Size([768, 12, 64]) tensor(0.2215)\n",
            "torch.Size([768, 12, 64]) tensor(0.2410)\n",
            "torch.Size([768, 12, 64]) tensor(13.5678)\n",
            "torch.Size([768, 12, 64]) tensor(-6.3677)\n",
            "torch.Size([768, 12, 64]) tensor(-7.7339)\n",
            "torch.Size([12, 64]) tensor(-0.0598)\n",
            "torch.Size([12, 64]) tensor(0.0004)\n",
            "torch.Size([12, 64]) tensor(0.0528)\n",
            "torch.Size([2, 12, 64]) tensor(3.3702e-08)\n",
            "torch.Size([768]) tensor(-0.1572)\n",
            "torch.Size([768]) tensor(0.0150)\n",
            "torch.Size([768]) tensor(0.1397)\n",
            "torch.Size([768]) tensor(0.5654)\n",
            "torch.Size([3072, 768]) tensor(-1.1589)\n",
            "torch.Size([3072]) tensor(0.0726)\n",
            "torch.Size([768, 3072]) tensor(18.7055)\n",
            "torch.Size([768]) tensor(-0.1558)\n",
            "torch.Size([768, 12, 64]) tensor(2.4332)\n",
            "torch.Size([768, 12, 64]) tensor(0.1259)\n",
            "torch.Size([768, 12, 64]) tensor(-3.6672)\n",
            "torch.Size([768, 12, 64]) tensor(0.1603)\n",
            "torch.Size([768, 12, 64]) tensor(0.3569)\n",
            "torch.Size([12, 64]) tensor(-0.0696)\n",
            "torch.Size([12, 64]) tensor(-0.0003)\n",
            "torch.Size([12, 64]) tensor(-0.0196)\n",
            "torch.Size([2, 12, 64]) tensor(8.5201e-09)\n",
            "torch.Size([768]) tensor(-0.4144)\n",
            "torch.Size([768]) tensor(-0.1806)\n",
            "torch.Size([768]) tensor(0.0853)\n",
            "torch.Size([768]) tensor(0.8793)\n",
            "torch.Size([3072, 768]) tensor(-0.0659)\n",
            "torch.Size([3072]) tensor(0.0592)\n",
            "torch.Size([768, 3072]) tensor(-0.0046)\n",
            "torch.Size([768]) tensor(0.0250)\n",
            "torch.Size([768, 12, 64]) tensor(2.6910)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0323)\n",
            "torch.Size([768, 12, 64]) tensor(-19.4874)\n",
            "torch.Size([768, 12, 64]) tensor(1.0631)\n",
            "torch.Size([768, 12, 64]) tensor(-6.1218)\n",
            "torch.Size([12, 64]) tensor(-0.0287)\n",
            "torch.Size([12, 64]) tensor(0.0004)\n",
            "torch.Size([12, 64]) tensor(-0.0431)\n",
            "torch.Size([2, 12, 64]) tensor(-4.0352e-08)\n",
            "torch.Size([768]) tensor(0.1026)\n",
            "torch.Size([768]) tensor(0.0330)\n",
            "torch.Size([768]) tensor(-0.1339)\n",
            "torch.Size([768]) tensor(-0.1971)\n",
            "torch.Size([3072, 768]) tensor(-1.5187)\n",
            "torch.Size([3072]) tensor(0.2006)\n",
            "torch.Size([768, 3072]) tensor(41.2691)\n",
            "torch.Size([768]) tensor(-0.2632)\n",
            "torch.Size([768, 12, 64]) tensor(-3.3266)\n",
            "torch.Size([768, 12, 64]) tensor(0.0111)\n",
            "torch.Size([768, 12, 64]) tensor(3.4266)\n",
            "torch.Size([768, 12, 64]) tensor(-2.5763)\n",
            "torch.Size([768, 12, 64]) tensor(5.2791)\n",
            "torch.Size([12, 64]) tensor(0.0740)\n",
            "torch.Size([12, 64]) tensor(-0.0089)\n",
            "torch.Size([12, 64]) tensor(0.0597)\n",
            "torch.Size([2, 12, 64]) tensor(1.9229e-08)\n",
            "torch.Size([768]) tensor(0.0498)\n",
            "torch.Size([768]) tensor(0.2423)\n",
            "torch.Size([768]) tensor(0.1578)\n",
            "torch.Size([768]) tensor(1.7394)\n",
            "torch.Size([3072, 768]) tensor(0.3741)\n",
            "torch.Size([3072]) tensor(-0.1578)\n",
            "torch.Size([768, 3072]) tensor(31.6754)\n",
            "torch.Size([768]) tensor(-0.1860)\n",
            "torch.Size([768, 12, 64]) tensor(-2.6449)\n",
            "torch.Size([768, 12, 64]) tensor(-0.1301)\n",
            "torch.Size([768, 12, 64]) tensor(-7.1596)\n",
            "torch.Size([768, 12, 64]) tensor(-0.1728)\n",
            "torch.Size([768, 12, 64]) tensor(2.9997)\n",
            "torch.Size([12, 64]) tensor(-0.0086)\n",
            "torch.Size([12, 64]) tensor(-0.0092)\n",
            "torch.Size([12, 64]) tensor(0.0573)\n",
            "torch.Size([2, 12, 64]) tensor(4.7439e-08)\n",
            "torch.Size([768]) tensor(-0.0852)\n",
            "torch.Size([768]) tensor(0.4079)\n",
            "torch.Size([768]) tensor(0.1847)\n",
            "torch.Size([768]) tensor(-0.1882)\n",
            "torch.Size([3072, 768]) tensor(0.8836)\n",
            "torch.Size([3072]) tensor(-0.1729)\n",
            "torch.Size([768, 3072]) tensor(-2.6295)\n",
            "torch.Size([768]) tensor(0.0467)\n",
            "torch.Size([768, 12, 64]) tensor(-4.4838)\n",
            "torch.Size([768, 12, 64]) tensor(-0.1135)\n",
            "torch.Size([768, 12, 64]) tensor(12.2713)\n",
            "torch.Size([768, 12, 64]) tensor(-0.3879)\n",
            "torch.Size([768, 12, 64]) tensor(-1.1210)\n",
            "torch.Size([12, 64]) tensor(-0.0097)\n",
            "torch.Size([12, 64]) tensor(-0.0037)\n",
            "torch.Size([12, 64]) tensor(0.0553)\n",
            "torch.Size([2, 12, 64]) tensor(-1.6838e-08)\n",
            "torch.Size([768]) tensor(-0.3964)\n",
            "torch.Size([768]) tensor(-1.9354)\n",
            "torch.Size([768]) tensor(-0.0974)\n",
            "torch.Size([768]) tensor(-0.0698)\n",
            "torch.Size([3072, 768]) tensor(-3.7310)\n",
            "torch.Size([3072]) tensor(0.1833)\n",
            "torch.Size([768, 3072]) tensor(35.5843)\n",
            "torch.Size([768]) tensor(0.0177)\n",
            "torch.Size([768, 768]) tensor(1.7760)\n",
            "torch.Size([768]) tensor(-0.0414)\n",
            "torch.Size([2, 768]) tensor(1.9033e-05)\n",
            "torch.Size([2]) tensor(-1.4901e-08)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGzaejkAb265",
        "colab_type": "text"
      },
      "source": [
        "### 例2. 抽取式问答(类似[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/))\n",
        "![image.png](https://raw.githubusercontent.com/qiaochen/NLPLecturePreparation/master/qa.PNG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMUj6eqKb265",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AnsStartLogits(nn.Module):\n",
        "    \"\"\"\n",
        "    用于预测每个token是否为答案span开始位置\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, \n",
        "                hidden_states, \n",
        "                p_mask=None\n",
        "               ):\n",
        "        x = self.linear(hidden_states).squeeze(-1)\n",
        "\n",
        "        if p_mask is not None:\n",
        "            x = x * (1 - p_mask) - 1e30 * p_mask\n",
        "        return x\n",
        "    \n",
        "    \n",
        "class AnsEndLogits(nn.Module):\n",
        "    \"\"\"\n",
        "    用于预测每个token是否为答案span结束位置，符合直觉，conditioned on 开始位置\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "                nn.Linear(hidden_size * 2, hidden_size),\n",
        "                nn.Tanh(),\n",
        "                nn.LayerNorm(hidden_size),\n",
        "                nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "                hidden_states,\n",
        "                start_states,\n",
        "                p_mask = None,\n",
        "               ):\n",
        "\n",
        "        x = self.layer(torch.cat([hidden_states, start_states], dim=-1))\n",
        "        x = x.squeeze(-1)\n",
        "\n",
        "        if p_mask is not None:\n",
        "            x = x * (1 - p_mask) - 1e30 * p_mask\n",
        "        return x\n",
        "    \n",
        "\n",
        "class XLNetQuestionAnswering(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 num_labels,\n",
        "                 xlnet_model,\n",
        "                 d_model=768,\n",
        "                 top_k_start=2,\n",
        "                 top_k_end=2\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.transformer = xlnet_model\n",
        "        self.start_logits = AnsStartLogits(d_model)\n",
        "        self.end_logits = AnsEndLogits(d_model)\n",
        "        self.top_k_start = top_k_start # for beam search\n",
        "        self.top_k_end = top_k_end # for beam search       \n",
        "        \n",
        "    def forward(self, \n",
        "                model_inputs,\n",
        "                p_mask=None,\n",
        "                start_positions=None\n",
        "                ):\n",
        "        \"\"\"\n",
        "        p_mask:\n",
        "            可选的mask, 被mask掉的位置不可能存在答案(e.g. [CLS], [PAD], ...)。\n",
        "            1.0 表示应当被mask. 0.0反之。\n",
        "        start_positions:\n",
        "            正确答案标注的开始位置。训练时需要输入模型以利用teacher forcing计算end_logits。\n",
        "            Inference时不需输入，beam search返回top k个开始和结束位置。\n",
        "        \"\"\"\n",
        "        transformer_outputs = self.transformer(**model_inputs)\n",
        "        \n",
        "        hidden_states = transformer_outputs[0]\n",
        "        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n",
        "        \n",
        "        if not start_positions is None:\n",
        "            # 在训练时利用 teacher forcing trick训练 end_logits\n",
        "            slen, hsz = hidden_states.shape[-2:]\n",
        "            start_positions = start_positions.expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n",
        "            start_states = hidden_states.gather(-2, start_positions)  # shape (bsz, 1, hsz)\n",
        "            start_states = start_states.expand(-1, slen, -1)  # shape (bsz, slen, hsz)\n",
        "            end_logits = self.end_logits(hidden_states, \n",
        "                                         start_states=start_states, \n",
        "                                         p_mask=p_mask)\n",
        "            \n",
        "            return start_logits, end_logits\n",
        "        else:\n",
        "            # 在Inference时利用Beam Search求end_logits\n",
        "            bsz, slen, hsz = hidden_states.size()\n",
        "            start_probs = torch.softmax(start_logits, dim=-1)  # shape (bsz, slen)\n",
        "\n",
        "            start_top_probs, start_top_index = torch.topk(\n",
        "                start_probs, self.top_k_start, dim=-1\n",
        "            )  # shape (bsz, top_k_start)\n",
        "            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz)  # shape (bsz, top_k_start, hsz)\n",
        "            start_states = torch.gather(hidden_states, -2, start_top_index_exp)  # shape (bsz, top_k_start, hsz)\n",
        "            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1)  # shape (bsz, slen, top_k_start, hsz)\n",
        "\n",
        "            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(\n",
        "                start_states\n",
        "            )  # shape (bsz, slen, top_k_start, hsz)\n",
        "            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n",
        "            end_logits = self.end_logits(hidden_states_expanded, \n",
        "                                         start_states=start_states, \n",
        "                                         p_mask=p_mask) \n",
        "            end_probs = torch.softmax(end_logits, dim=1)  # shape (bsz, slen, top_k_start)\n",
        "\n",
        "            end_top_probs, end_top_index = torch.topk(\n",
        "                end_probs, self.top_k_end, dim=1\n",
        "            )  # shape (bsz, top_k_end, top_k_start)\n",
        "            end_top_probs = end_top_probs.view(-1, self.top_k_start * self.top_k_end)\n",
        "            end_top_index = end_top_index.view(-1, self.top_k_start * self.top_k_end)\n",
        "            \n",
        "            return start_top_probs, start_top_index, end_top_probs, end_top_index, start_logits, end_logits\n",
        "    \n",
        "def get_loss(criterion, \n",
        "             start_logits, \n",
        "             start_positions,\n",
        "             end_logits,\n",
        "             end_positions\n",
        "            ):\n",
        "    start_loss = criterion(start_logits, start_positions)\n",
        "    end_loss = criterion(end_logits, end_positions)\n",
        "    return (start_loss + end_loss) / 2\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOMx3Ls5V7Us",
        "colab_type": "text"
      },
      "source": [
        "### 检测用于训练的forward和backward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgCYxpufb267",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context = r\"\"\"\n",
        "    Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "    architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
        "    Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "    TensorFlow 2.0 and PyTorch.\n",
        "    \"\"\"\n",
        "questions = [\n",
        "    \"How many pretrained models are available in Transformers?\",\n",
        "    \"What does Transformers provide?\",\n",
        "    \"Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "\n",
        "start_positions = torch.LongTensor([95, 36, 110])\n",
        "end_positions = torch.LongTensor([97, 88, 123])\n",
        "p_mask = [[1]*12 + [0]* (125 -14) + [1,1],\n",
        "          [1]* 7 + [0]* (120 - 9) + [1,1],\n",
        "          [1]*12 + [0]* (125 -14) + [1,1],\n",
        "         ]\n",
        "\n",
        "neg_log_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "q_answer = XLNetQuestionAnswering(2, model, 768, 2, 2)\n",
        "\n",
        "optimizer = torch.optim.AdamW(q_answer.parameters())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjez3p3ox5ou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "541074e9-9cfd-4012-8347-7e1b12021388"
      },
      "source": [
        "q_answer.train()\n",
        "optimizer.zero_grad()\n",
        "for ith, question in enumerate(questions):\n",
        "    start_logits, end_logits = q_answer(\n",
        "        tokenizer(question, \n",
        "                  context, \n",
        "                  add_special_tokens=True,\n",
        "                  return_tensors='pt'),\n",
        "        p_mask=torch.ByteTensor(p_mask[ith]),\n",
        "        start_positions=start_positions[ith].view(1,1,1)\n",
        "    )\n",
        "    loss = get_loss(\n",
        "        criterion,\n",
        "        start_logits, \n",
        "        start_positions[ith].view(-1),\n",
        "        end_logits,\n",
        "        end_positions[ith].view(-1)\n",
        "    )\n",
        "    print(\"\\nTrue Start: {}, True End: {}\\nPred Start Prob: {}, Pred End Prob: {}\\nPred Max Start: {}, Pred Max End: {}\\nPred Max Start Prob: {}, Pred Max end Prob:{}\\nLoss: {}\\n\".format(\n",
        "        start_positions[ith].item(),\n",
        "        end_positions[ith].item(),\n",
        "        torch.sigmoid(start_logits[:,start_positions[ith]]).item(), \n",
        "        torch.sigmoid(end_logits[:, end_positions[ith]]).item(),\n",
        "        torch.argmax(start_logits).item(),\n",
        "        torch.argmax(end_logits).item(),\n",
        "        torch.sigmoid(torch.max(start_logits)).item(), \n",
        "        torch.sigmoid(torch.max(end_logits)).item(),\n",
        "        loss.item()\n",
        "        )\n",
        "    )\n",
        "    print(\"=\"*25)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"\\nConfirm that the gradients are computed for the original XLNet parameters.\")\n",
        "for param in q_answer.parameters():\n",
        "    print(param.shape, param.grad.sum() if not param.grad is None else param.grad)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "True Start: 95, True End: 97\n",
            "Pred Start Prob: 0.9247106313705444, Pred End Prob: 0.48027488589286804\n",
            "Pred Max Start: 34, Pred Max End: 63\n",
            "Pred Max Start Prob: 0.9991466999053955, Pred Max end Prob:0.6437909007072449\n",
            "Loss: 6.035372734069824\n",
            "\n",
            "=========================\n",
            "\n",
            "True Start: 36, True End: 88\n",
            "Pred Start Prob: 0.9999798536300659, Pred End Prob: 0.3708997368812561\n",
            "Pred Max Start: 72, Pred Max End: 113\n",
            "Pred Max Start Prob: 0.9999934434890747, Pred Max end Prob:0.4424387216567993\n",
            "Loss: 4.303415775299072\n",
            "\n",
            "=========================\n",
            "\n",
            "True Start: 110, True End: 123\n",
            "Pred Start Prob: 0.9942066669464111, Pred End Prob: 0.0\n",
            "Pred Max Start: 100, Pred Max End: 63\n",
            "Pred Max Start Prob: 0.9999957084655762, Pred Max end Prob:0.6812169551849365\n",
            "Loss: 5.000000075237331e+29\n",
            "\n",
            "=========================\n",
            "\n",
            "Confirm that the gradients are computed for the original XLNet parameters.\n",
            "torch.Size([1, 1, 768]) None\n",
            "torch.Size([32000, 768]) tensor(0.6382)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0701)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0038)\n",
            "torch.Size([768, 12, 64]) tensor(0.6484)\n",
            "torch.Size([768, 12, 64]) tensor(0.4805)\n",
            "torch.Size([768, 12, 64]) tensor(0.9859)\n",
            "torch.Size([12, 64]) tensor(0.1264)\n",
            "torch.Size([12, 64]) tensor(0.0080)\n",
            "torch.Size([12, 64]) tensor(0.0898)\n",
            "torch.Size([2, 12, 64]) tensor(-2.0300e-09)\n",
            "torch.Size([768]) tensor(0.1217)\n",
            "torch.Size([768]) tensor(-0.0434)\n",
            "torch.Size([768]) tensor(-1.0349)\n",
            "torch.Size([768]) tensor(1.3181)\n",
            "torch.Size([3072, 768]) tensor(-0.8958)\n",
            "torch.Size([3072]) tensor(0.2688)\n",
            "torch.Size([768, 3072]) tensor(2.9374)\n",
            "torch.Size([768]) tensor(-0.0044)\n",
            "torch.Size([768, 12, 64]) tensor(0.4423)\n",
            "torch.Size([768, 12, 64]) tensor(-1.0353)\n",
            "torch.Size([768, 12, 64]) tensor(-0.6931)\n",
            "torch.Size([768, 12, 64]) tensor(-0.3406)\n",
            "torch.Size([768, 12, 64]) tensor(4.4671)\n",
            "torch.Size([12, 64]) tensor(0.0152)\n",
            "torch.Size([12, 64]) tensor(0.0031)\n",
            "torch.Size([12, 64]) tensor(-0.0058)\n",
            "torch.Size([2, 12, 64]) tensor(2.1836e-08)\n",
            "torch.Size([768]) tensor(-0.3602)\n",
            "torch.Size([768]) tensor(0.1196)\n",
            "torch.Size([768]) tensor(0.4814)\n",
            "torch.Size([768]) tensor(-3.0217)\n",
            "torch.Size([3072, 768]) tensor(-2.2750)\n",
            "torch.Size([3072]) tensor(-0.2830)\n",
            "torch.Size([768, 3072]) tensor(5.6878)\n",
            "torch.Size([768]) tensor(-0.0676)\n",
            "torch.Size([768, 12, 64]) tensor(0.0882)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0496)\n",
            "torch.Size([768, 12, 64]) tensor(0.5568)\n",
            "torch.Size([768, 12, 64]) tensor(-1.7731)\n",
            "torch.Size([768, 12, 64]) tensor(1.5736)\n",
            "torch.Size([12, 64]) tensor(0.0060)\n",
            "torch.Size([12, 64]) tensor(0.0077)\n",
            "torch.Size([12, 64]) tensor(0.0119)\n",
            "torch.Size([2, 12, 64]) tensor(-5.0859e-09)\n",
            "torch.Size([768]) tensor(0.4835)\n",
            "torch.Size([768]) tensor(0.0262)\n",
            "torch.Size([768]) tensor(1.2241)\n",
            "torch.Size([768]) tensor(-0.2372)\n",
            "torch.Size([3072, 768]) tensor(1.9969)\n",
            "torch.Size([3072]) tensor(0.4000)\n",
            "torch.Size([768, 3072]) tensor(-2.8167)\n",
            "torch.Size([768]) tensor(0.0168)\n",
            "torch.Size([768, 12, 64]) tensor(0.7827)\n",
            "torch.Size([768, 12, 64]) tensor(0.2636)\n",
            "torch.Size([768, 12, 64]) tensor(-1.6568)\n",
            "torch.Size([768, 12, 64]) tensor(-0.3490)\n",
            "torch.Size([768, 12, 64]) tensor(-2.0193)\n",
            "torch.Size([12, 64]) tensor(0.0126)\n",
            "torch.Size([12, 64]) tensor(0.0031)\n",
            "torch.Size([12, 64]) tensor(-0.1171)\n",
            "torch.Size([2, 12, 64]) tensor(-1.9591e-09)\n",
            "torch.Size([768]) tensor(0.3855)\n",
            "torch.Size([768]) tensor(-0.0531)\n",
            "torch.Size([768]) tensor(-0.3708)\n",
            "torch.Size([768]) tensor(-0.0791)\n",
            "torch.Size([3072, 768]) tensor(-7.2024)\n",
            "torch.Size([3072]) tensor(0.6853)\n",
            "torch.Size([768, 3072]) tensor(-0.7449)\n",
            "torch.Size([768]) tensor(0.0211)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0284)\n",
            "torch.Size([768, 12, 64]) tensor(0.0881)\n",
            "torch.Size([768, 12, 64]) tensor(12.4569)\n",
            "torch.Size([768, 12, 64]) tensor(0.1268)\n",
            "torch.Size([768, 12, 64]) tensor(-1.4205)\n",
            "torch.Size([12, 64]) tensor(0.0259)\n",
            "torch.Size([12, 64]) tensor(0.0028)\n",
            "torch.Size([12, 64]) tensor(-0.0319)\n",
            "torch.Size([2, 12, 64]) tensor(-3.9654e-10)\n",
            "torch.Size([768]) tensor(0.2548)\n",
            "torch.Size([768]) tensor(-0.0306)\n",
            "torch.Size([768]) tensor(-0.1362)\n",
            "torch.Size([768]) tensor(-0.8498)\n",
            "torch.Size([3072, 768]) tensor(-5.5363)\n",
            "torch.Size([3072]) tensor(0.3638)\n",
            "torch.Size([768, 3072]) tensor(-0.1899)\n",
            "torch.Size([768]) tensor(0.0014)\n",
            "torch.Size([768, 12, 64]) tensor(-0.5220)\n",
            "torch.Size([768, 12, 64]) tensor(-0.1435)\n",
            "torch.Size([768, 12, 64]) tensor(2.8374)\n",
            "torch.Size([768, 12, 64]) tensor(-0.4341)\n",
            "torch.Size([768, 12, 64]) tensor(-0.1202)\n",
            "torch.Size([12, 64]) tensor(0.0150)\n",
            "torch.Size([12, 64]) tensor(-0.0020)\n",
            "torch.Size([12, 64]) tensor(0.0208)\n",
            "torch.Size([2, 12, 64]) tensor(-6.0572e-10)\n",
            "torch.Size([768]) tensor(-0.7726)\n",
            "torch.Size([768]) tensor(0.2555)\n",
            "torch.Size([768]) tensor(-0.3458)\n",
            "torch.Size([768]) tensor(1.2133)\n",
            "torch.Size([3072, 768]) tensor(18.1768)\n",
            "torch.Size([3072]) tensor(-0.8609)\n",
            "torch.Size([768, 3072]) tensor(2.4702)\n",
            "torch.Size([768]) tensor(-0.0368)\n",
            "torch.Size([768, 12, 64]) tensor(0.8891)\n",
            "torch.Size([768, 12, 64]) tensor(-1.8738e-06)\n",
            "torch.Size([768, 12, 64]) tensor(-7.3541)\n",
            "torch.Size([768, 12, 64]) tensor(-2.1836)\n",
            "torch.Size([768, 12, 64]) tensor(6.1640)\n",
            "torch.Size([12, 64]) tensor(0.0075)\n",
            "torch.Size([12, 64]) tensor(0.0027)\n",
            "torch.Size([12, 64]) tensor(-0.0359)\n",
            "torch.Size([2, 12, 64]) tensor(-4.6748e-09)\n",
            "torch.Size([768]) tensor(0.9066)\n",
            "torch.Size([768]) tensor(-0.3301)\n",
            "torch.Size([768]) tensor(0.0190)\n",
            "torch.Size([768]) tensor(1.6005)\n",
            "torch.Size([3072, 768]) tensor(-3.7075)\n",
            "torch.Size([3072]) tensor(0.2724)\n",
            "torch.Size([768, 3072]) tensor(0.3947)\n",
            "torch.Size([768]) tensor(0.0039)\n",
            "torch.Size([768, 12, 64]) tensor(-0.7560)\n",
            "torch.Size([768, 12, 64]) tensor(0.0328)\n",
            "torch.Size([768, 12, 64]) tensor(-20.9544)\n",
            "torch.Size([768, 12, 64]) tensor(-0.5801)\n",
            "torch.Size([768, 12, 64]) tensor(-2.2308)\n",
            "torch.Size([12, 64]) tensor(0.0015)\n",
            "torch.Size([12, 64]) tensor(-0.0016)\n",
            "torch.Size([12, 64]) tensor(0.0252)\n",
            "torch.Size([2, 12, 64]) tensor(5.4325e-08)\n",
            "torch.Size([768]) tensor(0.1655)\n",
            "torch.Size([768]) tensor(-0.0862)\n",
            "torch.Size([768]) tensor(-0.0601)\n",
            "torch.Size([768]) tensor(2.6421)\n",
            "torch.Size([3072, 768]) tensor(5.2950)\n",
            "torch.Size([3072]) tensor(-0.2715)\n",
            "torch.Size([768, 3072]) tensor(-0.9539)\n",
            "torch.Size([768]) tensor(0.0953)\n",
            "torch.Size([768, 12, 64]) tensor(0.3832)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0556)\n",
            "torch.Size([768, 12, 64]) tensor(35.4843)\n",
            "torch.Size([768, 12, 64]) tensor(2.4660)\n",
            "torch.Size([768, 12, 64]) tensor(10.4205)\n",
            "torch.Size([12, 64]) tensor(-0.0395)\n",
            "torch.Size([12, 64]) tensor(0.0017)\n",
            "torch.Size([12, 64]) tensor(0.0280)\n",
            "torch.Size([2, 12, 64]) tensor(-1.0149e-07)\n",
            "torch.Size([768]) tensor(-0.1014)\n",
            "torch.Size([768]) tensor(0.4259)\n",
            "torch.Size([768]) tensor(0.1159)\n",
            "torch.Size([768]) tensor(-0.1899)\n",
            "torch.Size([3072, 768]) tensor(-0.9937)\n",
            "torch.Size([3072]) tensor(0.1919)\n",
            "torch.Size([768, 3072]) tensor(-3.1498)\n",
            "torch.Size([768]) tensor(0.0709)\n",
            "torch.Size([768, 12, 64]) tensor(0.3309)\n",
            "torch.Size([768, 12, 64]) tensor(0.0296)\n",
            "torch.Size([768, 12, 64]) tensor(3.1558)\n",
            "torch.Size([768, 12, 64]) tensor(1.7900)\n",
            "torch.Size([768, 12, 64]) tensor(-8.1443)\n",
            "torch.Size([12, 64]) tensor(-0.0414)\n",
            "torch.Size([12, 64]) tensor(-0.0016)\n",
            "torch.Size([12, 64]) tensor(0.0296)\n",
            "torch.Size([2, 12, 64]) tensor(-2.3188e-08)\n",
            "torch.Size([768]) tensor(-0.0444)\n",
            "torch.Size([768]) tensor(-0.4346)\n",
            "torch.Size([768]) tensor(0.3124)\n",
            "torch.Size([768]) tensor(2.5899)\n",
            "torch.Size([3072, 768]) tensor(-0.2512)\n",
            "torch.Size([3072]) tensor(0.2371)\n",
            "torch.Size([768, 3072]) tensor(10.0984)\n",
            "torch.Size([768]) tensor(-0.2578)\n",
            "torch.Size([768, 12, 64]) tensor(4.4705)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0339)\n",
            "torch.Size([768, 12, 64]) tensor(39.7858)\n",
            "torch.Size([768, 12, 64]) tensor(-3.8012)\n",
            "torch.Size([768, 12, 64]) tensor(-6.7427)\n",
            "torch.Size([12, 64]) tensor(-0.0142)\n",
            "torch.Size([12, 64]) tensor(0.0021)\n",
            "torch.Size([12, 64]) tensor(-0.0570)\n",
            "torch.Size([2, 12, 64]) tensor(1.1836e-07)\n",
            "torch.Size([768]) tensor(0.0622)\n",
            "torch.Size([768]) tensor(1.3210)\n",
            "torch.Size([768]) tensor(0.0381)\n",
            "torch.Size([768]) tensor(0.2080)\n",
            "torch.Size([3072, 768]) tensor(-5.7005)\n",
            "torch.Size([3072]) tensor(0.8143)\n",
            "torch.Size([768, 3072]) tensor(-35.6856)\n",
            "torch.Size([768]) tensor(-0.4428)\n",
            "torch.Size([768, 12, 64]) tensor(5.7817)\n",
            "torch.Size([768, 12, 64]) tensor(-0.3269)\n",
            "torch.Size([768, 12, 64]) tensor(-38.1101)\n",
            "torch.Size([768, 12, 64]) tensor(-10.8833)\n",
            "torch.Size([768, 12, 64]) tensor(-3.8934)\n",
            "torch.Size([12, 64]) tensor(-0.0350)\n",
            "torch.Size([12, 64]) tensor(-0.0013)\n",
            "torch.Size([12, 64]) tensor(-0.0197)\n",
            "torch.Size([2, 12, 64]) tensor(-2.2752e-07)\n",
            "torch.Size([768]) tensor(0.0455)\n",
            "torch.Size([768]) tensor(9.1592)\n",
            "torch.Size([768]) tensor(0.3740)\n",
            "torch.Size([768]) tensor(0.2764)\n",
            "torch.Size([3072, 768]) tensor(22.0342)\n",
            "torch.Size([3072]) tensor(-1.5590)\n",
            "torch.Size([768, 3072]) tensor(-35.2515)\n",
            "torch.Size([768]) tensor(-0.0788)\n",
            "torch.Size([1, 768]) tensor(105.4578)\n",
            "torch.Size([1]) tensor(2.3314e-08)\n",
            "torch.Size([768, 1536]) tensor(18.6142)\n",
            "torch.Size([768]) tensor(0.0967)\n",
            "torch.Size([768]) tensor(0.3233)\n",
            "torch.Size([768]) tensor(-0.4646)\n",
            "torch.Size([1, 768]) tensor(-0.0348)\n",
            "torch.Size([1]) tensor(0.5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urqElauRWKyF",
        "colab_type": "text"
      },
      "source": [
        "### inference的forward以及实现Beam Search decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ-9DxmHHAsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def decode(start_probs, end_probs, topk):\n",
        "    \"\"\"\n",
        "    给定beam中预测的开始和结束概率，搜索topk个最佳答案\n",
        "    \"\"\"\n",
        "    top_k_start = start_probs.shape[-1]\n",
        "    top_k_end = end_probs.shape[-1] // top_k_start\n",
        "\n",
        "    # 计算每一个(start, end)对的分数 P(start, end| sentence) = P(start|sentence) * P(end|start, sentence)\n",
        "    joint_probs = dict()\n",
        "    for i in range(top_k_start):\n",
        "      for j in range(top_k_end):\n",
        "        joint_probs[(i, j)] = start_probs[i]*end_probs[i*top_k_end+j]\n",
        "    \n",
        "    id_pairs, probs = zip(*sorted(joint_probs.items(), key=lambda kv:kv[1], reverse=True)[:topk])\n",
        "    start_ids, end_ids = zip(*id_pairs)\n",
        "    return start_ids, end_ids, probs\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXzlKiGKb269",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "3d7e19c2-3b26-44a7-842d-e303112380f9"
      },
      "source": [
        "# inference\n",
        "context = r\"\"\"\n",
        "    Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "    architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
        "    Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "    TensorFlow 2.0 and PyTorch.\n",
        "    \"\"\"\n",
        "questions = [\n",
        "    \"How many pretrained models are available in Transformers?\",\n",
        "    \"What does Transformers provide?\",\n",
        "    \"Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "q_answer.eval()\n",
        "for ith, question in enumerate(questions):\n",
        "    inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "    \n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    start_probs, start_index, end_probs, end_index, stt_logits, end_logits = q_answer(\n",
        "        inputs, \n",
        "        p_mask=torch.ByteTensor(p_mask[ith])\n",
        "    )\n",
        "\n",
        "    pred_starts, pred_ends, probs = decode(\n",
        "        start_probs.detach().squeeze().numpy(), \n",
        "        end_probs.detach().squeeze().numpy(), \n",
        "        2)\n",
        "    \n",
        "    # 只打印一个答案\n",
        "    start = start_index[:, pred_starts[0]].item()\n",
        "    end = end_index[:, pred_ends[0]].item()\n",
        "    \n",
        "#     print(probs, pred_starts, pred_ends)\n",
        "#     print(len(input_ids), stt_logits.shape, end_logits.shape)\n",
        "#     print(tokenizer.convert_ids_to_tokens(input_ids).index('?'))\n",
        "\n",
        "    print(\"=\"*25)\n",
        "    print(\"True start: {}, True end: {}\".format(\n",
        "        start_positions[ith].item(),\n",
        "        end_positions[ith].item()\n",
        "        ))\n",
        "    print(\"Max answer prob: {:0.8f}, start idx: {}, end idx: {}\".format(\n",
        "        probs[0],\n",
        "        start,\n",
        "        end,\n",
        "    ))\n",
        "    print(\"-\"*25)\n",
        "    print(\"Question: '{}'\".format(question))\n",
        "    print(\"Answer: '{}'\".format(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[start:end]))))\n",
        "    print(\"=\"*25)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================\n",
            "True start: 95, True end: 97\n",
            "Max answer prob: 0.00008147, start idx: 95, end idx: 101\n",
            "-------------------------\n",
            "Question: 'How many pretrained models are available in Transformers?'\n",
            "Answer: '32+ pretrained models in'\n",
            "=========================\n",
            "=========================\n",
            "True start: 36, True end: 88\n",
            "Max answer prob: 0.00008149, start idx: 90, end idx: 96\n",
            "-------------------------\n",
            "Question: 'What does Transformers provide?'\n",
            "Answer: '32+ pretrained models in'\n",
            "=========================\n",
            "=========================\n",
            "True start: 110, True end: 123\n",
            "Max answer prob: 0.00008148, start idx: 95, end idx: 101\n",
            "-------------------------\n",
            "Question: 'Transformers provides interoperability between which frameworks?'\n",
            "Answer: '32+ pretrained models in'\n",
            "=========================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwYZU9c4b26-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}