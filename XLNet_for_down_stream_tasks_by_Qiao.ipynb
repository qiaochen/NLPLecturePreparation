{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Huggingface_transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjB68KGLb26k",
        "colab_type": "text"
      },
      "source": [
        "## 利用[Huggingface](https://huggingface.co/transformers/installation.html#)实现的预训练语言模型做下游任务\n",
        "by Qiao for NLP7 2020-8-16\n",
        "\n",
        "预训练语言模型的用法：\n",
        "1. 作为特征提取器\n",
        "2. 作为encoder参与下游任务微调\n",
        "使用上非常类似，差别是后者在训练过程中原预训练语言模型的参数也允许优化。\n",
        "\n",
        "主要内容:\n",
        "1. 以XLNet介绍HuggingFace transformers组件的使用套路\n",
        "2. 以XLNet为例介绍如何接续下游的文本分类和抽取式问答。\n",
        "\n",
        "主要参考[文档](https://huggingface.co/transformers/model_doc/xlnet.html)和[代码](https://github.com/huggingface/transformers/blob/0ed7c00ba6b3178c8c323a0440bf1221fb99784b/src/transformers/modeling_xlnet.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dw21tvyb26k",
        "colab_type": "text"
      },
      "source": [
        "### 以[XLNet](https://huggingface.co/transformers/model_doc/xlnet.html)为例，使用其他Huggingface封装的预训练语言模型的套路与类似"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD69bu-ib26l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "867102b6-c602-4ec7-dd23-386a30ac6050"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "!pip install transformers\n",
        "from transformers import XLNetModel, XLNetTokenizer, XLNetConfig"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hf6yUBab26n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "model = XLNetModel.from_pretrained('xlnet-base-cased', \n",
        "                                   output_hidden_states=True,\n",
        "                                   output_attentions=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA4TQ7Xnb26p",
        "colab_type": "text"
      },
      "source": [
        "### 注意：\n",
        "以上使用模型名称初始化的模块，程序会在后台下载预训练完成的XLNet模型并加载。对于内地同学，除改变上网方式外，还可以手动下载模型，指定路径加载。\n",
        "#### 手动下载模型：\n",
        "在HuggingFace官方[模型库](https://huggingface.co/models)上找到需要下载的模型，点击模型链接，例如：[xlnet-base-cased](https://huggingface.co/xlnet-base-cased)模型。在跳转到的模型页面中点击`List all files in model`（字比较小，注意查看），将跳出框中的模型相关文(pytorch或tf版本)件保存到本地。\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSoaGwNlb26q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # 本地加载XLNet模型\n",
        "# MODEL_PATH = r\"D:\\data\\nlp\\xlnet-model/\"\n",
        "# config = XLNetConfig.from_json_file(os.path.join(MODEL_PATH, \"xlnet-base-cased-config.json\"))\n",
        "\n",
        "# #config文件不仅用于设置模型参数，也可以用来控制模型的行为\n",
        "# config.output_hidden_states = True\n",
        "# config.output_attentions = True\n",
        "\n",
        "# tokenizer = XLNetTokenizer(os.path.join(MODEL_PATH, 'xlnet-base-cased-spiece.model'))\n",
        "# model = XLNetModel.from_pretrained(MODEL_PATH, config = config)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwIhUM-gb26r",
        "colab_type": "text"
      },
      "source": [
        "### 1. 句子到token id转换"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF5mo_D5b26s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ca344d49-2bda-4387-b1ba-a76c40365003"
      },
      "source": [
        "# 利用tokenizer将原始的句子准备成模型输入\n",
        "sentence = \"This is an interesting review session\"\n",
        "\n",
        "# tokenization\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(\"Tokens: {}\".format(tokens))\n",
        "\n",
        "# 将token转化为ID\n",
        "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Tokens id: {}\".format(tokens_ids))\n",
        "\n",
        "# 添加特殊token: <cls>, <sep>\n",
        "tokens_ids = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
        "\n",
        "# 准备成pytorch tensor\n",
        "tokens_pt = torch.tensor([tokens_ids])\n",
        "print(\"Tokens PyTorch: {}\".format(tokens_pt))\n",
        "\n",
        "# print(tokenizer.convert_ids_to_tokens([122,   27,   48, 5272,  717,    4,    3]))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens: ['▁This', '▁is', '▁an', '▁interesting', '▁review', '▁session']\n",
            "Tokens id: [122, 27, 48, 2456, 1398, 1961]\n",
            "Tokens PyTorch: tensor([[ 122,   27,   48, 2456, 1398, 1961,    4,    3]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqhXQD9pb26u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "607ca07f-d684-4087-eba0-83df43af7032"
      },
      "source": [
        "# 偷懒的一条龙服务\n",
        "tokens_pt2 = tokenizer(sentence, return_tensors=\"pt\")\n",
        "print(\"Tokens PyTorch: {}\".format(tokens_pt2))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens PyTorch: {'input_ids': tensor([[ 122,   27,   48, 2456, 1398, 1961,    4,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rN4h6CFb26w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "851a0465-ebf8-42fb-9c9e-63e83c611034"
      },
      "source": [
        "# 批处理\n",
        "# padding\n",
        "sentences = [\"The ultimate answer to life, universe and time is 42.\", \"Take a towel for a space travel.\"]\n",
        "print(\"Batch tokenization:\\n\", tokenizer(sentences)['input_ids'])\n",
        "print(\"With Padding:\\n\", tokenizer(sentences, padding=True)['input_ids'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch tokenization:\n",
            " [[32, 6452, 1543, 22, 235, 19, 6486, 21, 92, 27, 4087, 9, 4, 3], [3636, 24, 14680, 28, 24, 888, 1316, 9, 4, 3]]\n",
            "With Padding:\n",
            " [[32, 6452, 1543, 22, 235, 19, 6486, 21, 92, 27, 4087, 9, 4, 3], [5, 5, 5, 5, 3636, 24, 14680, 28, 24, 888, 1316, 9, 4, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNv9Hw8Mb26y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c4695a53-e44d-47d7-d08e-02e7bc3d314c"
      },
      "source": [
        "# 输入句子对：\n",
        "multi_seg_input = tokenizer(\"This is segment A\", \"This is segment B\")\n",
        "print(\"Multi segment token (str): {}\".format(tokenizer.convert_ids_to_tokens(multi_seg_input['input_ids'])))\n",
        "print(\"Multi segment token (int): {}\".format(multi_seg_input['input_ids']))\n",
        "print(\"Multi segment type       : {}\".format(multi_seg_input['token_type_ids']))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multi segment token (str): ['▁This', '▁is', '▁segment', '▁A', '<sep>', '▁This', '▁is', '▁segment', '▁B', '<sep>', '<cls>']\n",
            "Multi segment token (int): [122, 27, 7295, 79, 4, 122, 27, 7295, 322, 4, 3]\n",
            "Multi segment type       : [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfjZ9f_wb260",
        "colab_type": "text"
      },
      "source": [
        "### 2. 模型encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxzgyb5jb260",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "adb286dd-ba79-4ac6-f15a-c0d24ed98942"
      },
      "source": [
        "# 默认情况下，model.dev()模式下。下面使用模型Encode输入的句子\n",
        "# 因为我们在config中设置模型返回每层的hidden states和注意力，再加上默认输出的最后一层隐状态，输出有3个部分\n",
        "print(\"Is training mode ? \", model.training)\n",
        "\n",
        "sentence = \"The ultimate answer to life, universe and time is 42.\"\n",
        "\n",
        "tokens_pt = tokenizer(sentence, return_tensors=\"pt\")\n",
        "print(\"Token (str): {}\".format(\n",
        "    tokenizer.convert_ids_to_tokens(tokens_pt['input_ids'][0])\n",
        "    ))\n",
        "\n",
        "final_layer_h, all_layer_h, attentions = model(**tokens_pt)\n",
        "\n",
        "print(torch.sum(final_layer_h - all_layer_h[-1]).item())\n",
        "\n",
        "final_layer_h.shape, len(all_layer_h), len(attentions)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is training mode ?  False\n",
            "Token (str): ['▁The', '▁ultimate', '▁answer', '▁to', '▁life', ',', '▁universe', '▁and', '▁time', '▁is', '▁42', '.', '<sep>', '<cls>']\n",
            "0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 14, 768]), 13, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbV9RUCcb262",
        "colab_type": "text"
      },
      "source": [
        "### 3. 下游任务\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDuKbswbb263",
        "colab_type": "text"
      },
      "source": [
        "### 例1. 文本分类"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Hl4iFib263",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class XLNetSeqSummary(nn.Module):\n",
        "    \n",
        "    def __init__(self, \n",
        "                 how='cls', \n",
        "                 hidden_size=768, \n",
        "                 activation=None, \n",
        "                 first_dropout=None, \n",
        "                 last_dropout=None):\n",
        "        super().__init__()\n",
        "        self.how = how\n",
        "        self.summary = nn.Linear(hidden_size, hidden_size)\n",
        "        self.activation = activation if activation else nn.GELU()\n",
        "        self.first_dropout = first_dropout if first_dropout else nn.Dropout(0.5)\n",
        "        self.last_dropout = last_dropout if last_dropout else nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        \"\"\"\n",
        "        对隐状态序列池化或返回cls处的表示，作为句子的encoding\n",
        "        Args:\n",
        "            hidden_states :\n",
        "                XLNet模型输出的最后层隐状态序列.\n",
        "        Returns:\n",
        "            : 句子向量表示\n",
        "        \"\"\"\n",
        "        if self.how == \"cls\":\n",
        "            output = hidden_states[:, -1]\n",
        "        elif self.how == \"mean\":\n",
        "            output = hidden_states.mean(dim=1)\n",
        "        elif self.how == \"max\":\n",
        "            output = hidden_states.max(dim=1)\n",
        "        else:\n",
        "            raise Exception(\"Summary type '{}' not implemted.\".format(self.how))\n",
        "\n",
        "        output = self.first_dropout(output)\n",
        "        output = self.summary(output)\n",
        "        output = self.activation(output)\n",
        "        output = self.last_dropout(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class XLNetSentenceEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 num_labels,\n",
        "                 xlnet_model,\n",
        "                 d_model=768):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.d_model = d_model\n",
        "        self.transformer = xlnet_model\n",
        "        self.sequence_summary = XLNetSeqSummary('cls', d_model, nn.GELU())\n",
        "        self.logits_proj = nn.Linear(d_model, num_labels)\n",
        "        \n",
        "    def forward(self, model_inputs):\n",
        "        transformer_outputs = self.transformer(**model_inputs)\n",
        "            \n",
        "        output = transformer_outputs[0]\n",
        "        output = self.sequence_summary(output)\n",
        "        logits = self.logits_proj(output)\n",
        "\n",
        "        return logits\n",
        "    \n",
        "def get_loss(criterion, logits, labels):\n",
        "    return criterion(logits, labels)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvLk2n2s11dm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a32b8eeb-6523-4610-dc41-6b28a1d1abd8"
      },
      "source": [
        "# 验证forward和反向传播\n",
        "\n",
        "# toy examples\n",
        "sentences = [\"The ultimate answer to life, universe and time is 42.\", \n",
        "             \"Take a towel for a space travel.\"]\n",
        "labels = torch.LongTensor([0, 1])\n",
        "\n",
        "# 实例化各个模块\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "encoder = XLNetSentenceEncoder(2, model, 768)\n",
        "optimizer = torch.optim.AdamW(encoder.parameters())\n",
        "\n",
        "# forward + loss\n",
        "encoder.train()\n",
        "optimizer.zero_grad()\n",
        "logits = encoder(tokenizer(sentences, padding=True, return_tensors='pt'))\n",
        "loss = get_loss(criterion, logits, labels)\n",
        "\n",
        "print(\"Loss: \", loss.item())\n",
        "\n",
        "# backwawrd step\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"=\"*25)\n",
        "print(\"Confirm that the gradients are computed for the original XLNet parameters.\\n\")\n",
        "for param in encoder.parameters():\n",
        "    print(param.shape, param.grad.sum() if not param.grad is None else param.grad)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_xlnet.py:283: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
            "  attn_score = (ac + bd + ef) * self.scale\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  0.6122478246688843\n",
            "Confirm that the gradients are computed for the original XLNet parameters.\n",
            "torch.Size([1, 1, 768]) None\n",
            "torch.Size([32000, 768]) tensor(-43.1771)\n",
            "torch.Size([768, 12, 64]) tensor(0.3310)\n",
            "torch.Size([768, 12, 64]) tensor(0.0162)\n",
            "torch.Size([768, 12, 64]) tensor(14.2157)\n",
            "torch.Size([768, 12, 64]) tensor(2.8287)\n",
            "torch.Size([768, 12, 64]) tensor(6.1598)\n",
            "torch.Size([12, 64]) tensor(0.8269)\n",
            "torch.Size([12, 64]) tensor(0.0587)\n",
            "torch.Size([12, 64]) tensor(0.1534)\n",
            "torch.Size([2, 12, 64]) tensor(2.3182e-08)\n",
            "torch.Size([768]) tensor(-1.1875)\n",
            "torch.Size([768]) tensor(0.1646)\n",
            "torch.Size([768]) tensor(12.1468)\n",
            "torch.Size([768]) tensor(-7.8845)\n",
            "torch.Size([3072, 768]) tensor(7.2710)\n",
            "torch.Size([3072]) tensor(-1.5695)\n",
            "torch.Size([768, 3072]) tensor(-107.8798)\n",
            "torch.Size([768]) tensor(0.2444)\n",
            "torch.Size([768, 12, 64]) tensor(-29.3908)\n",
            "torch.Size([768, 12, 64]) tensor(0.2864)\n",
            "torch.Size([768, 12, 64]) tensor(-3.2693)\n",
            "torch.Size([768, 12, 64]) tensor(-2.3721)\n",
            "torch.Size([768, 12, 64]) tensor(20.1100)\n",
            "torch.Size([12, 64]) tensor(-0.2168)\n",
            "torch.Size([12, 64]) tensor(-0.0084)\n",
            "torch.Size([12, 64]) tensor(0.4270)\n",
            "torch.Size([2, 12, 64]) tensor(1.5018e-08)\n",
            "torch.Size([768]) tensor(2.9177)\n",
            "torch.Size([768]) tensor(-1.4423)\n",
            "torch.Size([768]) tensor(0.0082)\n",
            "torch.Size([768]) tensor(-12.1623)\n",
            "torch.Size([3072, 768]) tensor(14.1586)\n",
            "torch.Size([3072]) tensor(1.9107)\n",
            "torch.Size([768, 3072]) tensor(-7.9894)\n",
            "torch.Size([768]) tensor(-0.0530)\n",
            "torch.Size([768, 12, 64]) tensor(-1.7352)\n",
            "torch.Size([768, 12, 64]) tensor(-0.6424)\n",
            "torch.Size([768, 12, 64]) tensor(-39.5001)\n",
            "torch.Size([768, 12, 64]) tensor(-3.1346)\n",
            "torch.Size([768, 12, 64]) tensor(-2.4973)\n",
            "torch.Size([12, 64]) tensor(0.0408)\n",
            "torch.Size([12, 64]) tensor(-0.0331)\n",
            "torch.Size([12, 64]) tensor(-0.0233)\n",
            "torch.Size([2, 12, 64]) tensor(2.2203e-07)\n",
            "torch.Size([768]) tensor(1.8329)\n",
            "torch.Size([768]) tensor(-0.3731)\n",
            "torch.Size([768]) tensor(-1.8869)\n",
            "torch.Size([768]) tensor(4.2029)\n",
            "torch.Size([3072, 768]) tensor(9.2081)\n",
            "torch.Size([3072]) tensor(-0.7119)\n",
            "torch.Size([768, 3072]) tensor(-39.1301)\n",
            "torch.Size([768]) tensor(0.2672)\n",
            "torch.Size([768, 12, 64]) tensor(1.0247)\n",
            "torch.Size([768, 12, 64]) tensor(1.1164)\n",
            "torch.Size([768, 12, 64]) tensor(-7.1904)\n",
            "torch.Size([768, 12, 64]) tensor(2.2829)\n",
            "torch.Size([768, 12, 64]) tensor(20.1392)\n",
            "torch.Size([12, 64]) tensor(-0.1372)\n",
            "torch.Size([12, 64]) tensor(-0.0504)\n",
            "torch.Size([12, 64]) tensor(0.2014)\n",
            "torch.Size([2, 12, 64]) tensor(-1.5544e-07)\n",
            "torch.Size([768]) tensor(0.3162)\n",
            "torch.Size([768]) tensor(-0.2473)\n",
            "torch.Size([768]) tensor(2.4896)\n",
            "torch.Size([768]) tensor(2.3315)\n",
            "torch.Size([3072, 768]) tensor(-1.7351)\n",
            "torch.Size([3072]) tensor(-0.3005)\n",
            "torch.Size([768, 3072]) tensor(-20.6219)\n",
            "torch.Size([768]) tensor(0.1645)\n",
            "torch.Size([768, 12, 64]) tensor(22.0696)\n",
            "torch.Size([768, 12, 64]) tensor(-1.9898)\n",
            "torch.Size([768, 12, 64]) tensor(11.0174)\n",
            "torch.Size([768, 12, 64]) tensor(0.5778)\n",
            "torch.Size([768, 12, 64]) tensor(-5.1477)\n",
            "torch.Size([12, 64]) tensor(-0.2052)\n",
            "torch.Size([12, 64]) tensor(0.0027)\n",
            "torch.Size([12, 64]) tensor(0.8539)\n",
            "torch.Size([2, 12, 64]) tensor(-1.0594e-08)\n",
            "torch.Size([768]) tensor(0.0169)\n",
            "torch.Size([768]) tensor(0.2698)\n",
            "torch.Size([768]) tensor(-1.4567)\n",
            "torch.Size([768]) tensor(4.7776)\n",
            "torch.Size([3072, 768]) tensor(26.2348)\n",
            "torch.Size([3072]) tensor(-1.0767)\n",
            "torch.Size([768, 3072]) tensor(2.4077)\n",
            "torch.Size([768]) tensor(-0.0945)\n",
            "torch.Size([768, 12, 64]) tensor(0.1913)\n",
            "torch.Size([768, 12, 64]) tensor(0.1016)\n",
            "torch.Size([768, 12, 64]) tensor(-8.1892)\n",
            "torch.Size([768, 12, 64]) tensor(2.8681)\n",
            "torch.Size([768, 12, 64]) tensor(8.0816)\n",
            "torch.Size([12, 64]) tensor(-0.0022)\n",
            "torch.Size([12, 64]) tensor(-0.0151)\n",
            "torch.Size([12, 64]) tensor(0.0023)\n",
            "torch.Size([2, 12, 64]) tensor(-3.5485e-08)\n",
            "torch.Size([768]) tensor(1.9777)\n",
            "torch.Size([768]) tensor(-0.7282)\n",
            "torch.Size([768]) tensor(0.6875)\n",
            "torch.Size([768]) tensor(4.7159)\n",
            "torch.Size([3072, 768]) tensor(-49.4218)\n",
            "torch.Size([3072]) tensor(1.7255)\n",
            "torch.Size([768, 3072]) tensor(-54.2702)\n",
            "torch.Size([768]) tensor(0.4277)\n",
            "torch.Size([768, 12, 64]) tensor(-7.0297)\n",
            "torch.Size([768, 12, 64]) tensor(0.1280)\n",
            "torch.Size([768, 12, 64]) tensor(-3.7681)\n",
            "torch.Size([768, 12, 64]) tensor(16.7945)\n",
            "torch.Size([768, 12, 64]) tensor(3.5810)\n",
            "torch.Size([12, 64]) tensor(0.1863)\n",
            "torch.Size([12, 64]) tensor(-0.0087)\n",
            "torch.Size([12, 64]) tensor(0.0326)\n",
            "torch.Size([2, 12, 64]) tensor(4.2506e-08)\n",
            "torch.Size([768]) tensor(-0.7245)\n",
            "torch.Size([768]) tensor(-0.2842)\n",
            "torch.Size([768]) tensor(-0.0505)\n",
            "torch.Size([768]) tensor(1.0159)\n",
            "torch.Size([3072, 768]) tensor(1.0924)\n",
            "torch.Size([3072]) tensor(-0.0379)\n",
            "torch.Size([768, 3072]) tensor(39.3315)\n",
            "torch.Size([768]) tensor(-0.3282)\n",
            "torch.Size([768, 12, 64]) tensor(-1.3221)\n",
            "torch.Size([768, 12, 64]) tensor(-0.1729)\n",
            "torch.Size([768, 12, 64]) tensor(-6.0528)\n",
            "torch.Size([768, 12, 64]) tensor(-5.6989)\n",
            "torch.Size([768, 12, 64]) tensor(5.1762)\n",
            "torch.Size([12, 64]) tensor(-0.0223)\n",
            "torch.Size([12, 64]) tensor(-0.0159)\n",
            "torch.Size([12, 64]) tensor(0.0812)\n",
            "torch.Size([2, 12, 64]) tensor(9.2768e-09)\n",
            "torch.Size([768]) tensor(-0.5857)\n",
            "torch.Size([768]) tensor(-1.5734)\n",
            "torch.Size([768]) tensor(-0.5537)\n",
            "torch.Size([768]) tensor(1.9847)\n",
            "torch.Size([3072, 768]) tensor(-31.4185)\n",
            "torch.Size([3072]) tensor(1.1698)\n",
            "torch.Size([768, 3072]) tensor(-12.0026)\n",
            "torch.Size([768]) tensor(0.0564)\n",
            "torch.Size([768, 12, 64]) tensor(10.7603)\n",
            "torch.Size([768, 12, 64]) tensor(0.1394)\n",
            "torch.Size([768, 12, 64]) tensor(47.4856)\n",
            "torch.Size([768, 12, 64]) tensor(-1.6505)\n",
            "torch.Size([768, 12, 64]) tensor(-2.6038)\n",
            "torch.Size([12, 64]) tensor(-0.2383)\n",
            "torch.Size([12, 64]) tensor(0.0008)\n",
            "torch.Size([12, 64]) tensor(-0.0522)\n",
            "torch.Size([2, 12, 64]) tensor(1.1735e-07)\n",
            "torch.Size([768]) tensor(0.2640)\n",
            "torch.Size([768]) tensor(-1.2024)\n",
            "torch.Size([768]) tensor(-0.2669)\n",
            "torch.Size([768]) tensor(-0.0203)\n",
            "torch.Size([3072, 768]) tensor(-6.2418)\n",
            "torch.Size([3072]) tensor(0.8397)\n",
            "torch.Size([768, 3072]) tensor(35.0819)\n",
            "torch.Size([768]) tensor(-0.2329)\n",
            "torch.Size([768, 12, 64]) tensor(-3.7773)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0446)\n",
            "torch.Size([768, 12, 64]) tensor(3.8490)\n",
            "torch.Size([768, 12, 64]) tensor(1.3348)\n",
            "torch.Size([768, 12, 64]) tensor(-6.3214)\n",
            "torch.Size([12, 64]) tensor(0.0574)\n",
            "torch.Size([12, 64]) tensor(-0.0060)\n",
            "torch.Size([12, 64]) tensor(0.0885)\n",
            "torch.Size([2, 12, 64]) tensor(1.1391e-08)\n",
            "torch.Size([768]) tensor(0.0747)\n",
            "torch.Size([768]) tensor(0.3964)\n",
            "torch.Size([768]) tensor(-0.2299)\n",
            "torch.Size([768]) tensor(-5.5317)\n",
            "torch.Size([3072, 768]) tensor(1.4737)\n",
            "torch.Size([3072]) tensor(-0.4838)\n",
            "torch.Size([768, 3072]) tensor(22.2405)\n",
            "torch.Size([768]) tensor(-0.1409)\n",
            "torch.Size([768, 12, 64]) tensor(16.2479)\n",
            "torch.Size([768, 12, 64]) tensor(0.2416)\n",
            "torch.Size([768, 12, 64]) tensor(-14.3603)\n",
            "torch.Size([768, 12, 64]) tensor(-0.1987)\n",
            "torch.Size([768, 12, 64]) tensor(-4.0180)\n",
            "torch.Size([12, 64]) tensor(0.0907)\n",
            "torch.Size([12, 64]) tensor(0.0040)\n",
            "torch.Size([12, 64]) tensor(-0.3438)\n",
            "torch.Size([2, 12, 64]) tensor(2.1540e-08)\n",
            "torch.Size([768]) tensor(0.3694)\n",
            "torch.Size([768]) tensor(-0.5489)\n",
            "torch.Size([768]) tensor(-0.3476)\n",
            "torch.Size([768]) tensor(-2.7788)\n",
            "torch.Size([3072, 768]) tensor(-3.5117)\n",
            "torch.Size([3072]) tensor(0.4661)\n",
            "torch.Size([768, 3072]) tensor(-21.7103)\n",
            "torch.Size([768]) tensor(0.2155)\n",
            "torch.Size([768, 12, 64]) tensor(10.5825)\n",
            "torch.Size([768, 12, 64]) tensor(0.7292)\n",
            "torch.Size([768, 12, 64]) tensor(33.1114)\n",
            "torch.Size([768, 12, 64]) tensor(0.2911)\n",
            "torch.Size([768, 12, 64]) tensor(2.2789)\n",
            "torch.Size([12, 64]) tensor(0.0227)\n",
            "torch.Size([12, 64]) tensor(-0.0101)\n",
            "torch.Size([12, 64]) tensor(-0.1027)\n",
            "torch.Size([2, 12, 64]) tensor(4.5926e-08)\n",
            "torch.Size([768]) tensor(-2.1944)\n",
            "torch.Size([768]) tensor(-3.6059)\n",
            "torch.Size([768]) tensor(-0.0259)\n",
            "torch.Size([768]) tensor(-0.0576)\n",
            "torch.Size([3072, 768]) tensor(8.2146)\n",
            "torch.Size([3072]) tensor(-0.5104)\n",
            "torch.Size([768, 3072]) tensor(18.6262)\n",
            "torch.Size([768]) tensor(-0.1347)\n",
            "torch.Size([768, 768]) tensor(-2.9858)\n",
            "torch.Size([768]) tensor(-0.2702)\n",
            "torch.Size([2, 768]) tensor(-3.2101e-06)\n",
            "torch.Size([2]) tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGzaejkAb265",
        "colab_type": "text"
      },
      "source": [
        "### 例2. 阅读理解式问答(类似[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMUj6eqKb265",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AnsStartLogits(nn.Module):\n",
        "    \"\"\"\n",
        "    用于预测每个token是否为答案span开始位置\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, \n",
        "                hidden_states, \n",
        "                p_mask=None\n",
        "               ):\n",
        "        x = self.linear(hidden_states).squeeze(-1)\n",
        "\n",
        "        if p_mask is not None:\n",
        "            x = x * (1 - p_mask) - 1e30 * p_mask\n",
        "        return x\n",
        "    \n",
        "    \n",
        "class AnsEndLogits(nn.Module):\n",
        "    \"\"\"\n",
        "    用于预测每个token是否为答案span结束位置，符合直觉，conditioned on 开始位置\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "                nn.Linear(hidden_size * 2, hidden_size),\n",
        "                nn.Tanh(),\n",
        "                nn.LayerNorm(hidden_size),\n",
        "                nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "                hidden_states,\n",
        "                start_states,\n",
        "                p_mask = None,\n",
        "               ):\n",
        "\n",
        "        x = self.layer(torch.cat([hidden_states, start_states], dim=-1))\n",
        "        x = x.squeeze(-1)\n",
        "\n",
        "        if p_mask is not None:\n",
        "            x = x * (1 - p_mask) - 1e30 * p_mask\n",
        "        return x\n",
        "    \n",
        "\n",
        "class XLNetQuestionEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 num_labels,\n",
        "                 xlnet_model,\n",
        "                 d_model=768,\n",
        "                 top_k_start=2,\n",
        "                 top_k_end=2\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.transformer = xlnet_model\n",
        "        self.start_logits = AnsStartLogits(d_model)\n",
        "        self.end_logits = AnsEndLogits(d_model)\n",
        "        self.top_k_start = top_k_start # for beam search\n",
        "        self.top_k_end = top_k_end # for beam search\n",
        "        \n",
        "    def forward(self, \n",
        "                model_inputs,\n",
        "                p_mask=None,\n",
        "                start_positions=None\n",
        "                ):\n",
        "        \"\"\"\n",
        "        p_mask:\n",
        "            可选的mask, 被mask掉的位置不可能存在答案(e.g. [CLS], [PAD], ...)。\n",
        "            1.0 表示应当被mask. 0.0反之。\n",
        "        start_positions:\n",
        "            正确答案标注的开始位置。训练时需要输入模型以利用teacher forcing计算end_logits。\n",
        "            Inference时不需输入，beam search返回top k个开始和结束位置。\n",
        "        \"\"\"\n",
        "        transformer_outputs = self.transformer(**model_inputs)\n",
        "        \n",
        "        hidden_states = transformer_outputs[0]\n",
        "        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n",
        "        \n",
        "        if not start_positions is None:\n",
        "            # 在训练时利用 teacher forcing trick训练 end_logits\n",
        "            slen, hsz = hidden_states.shape[-2:]\n",
        "            start_positions = start_positions.expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n",
        "            start_states = hidden_states.gather(-2, start_positions)  # shape (bsz, 1, hsz)\n",
        "            start_states = start_states.expand(-1, slen, -1)  # shape (bsz, slen, hsz)\n",
        "            end_logits = self.end_logits(hidden_states, \n",
        "                                         start_states=start_states, \n",
        "                                         p_mask=p_mask)\n",
        "            \n",
        "            return start_logits, end_logits\n",
        "        else:\n",
        "            # 在Inference时利用Beam Search求end_logits\n",
        "            bsz, slen, hsz = hidden_states.size()\n",
        "            start_probs = torch.softmax(start_logits, dim=-1)  # shape (bsz, slen)\n",
        "\n",
        "            start_top_probs, start_top_index = torch.topk(\n",
        "                start_probs, self.top_k_start, dim=-1\n",
        "            )  # shape (bsz, start_n_top)\n",
        "            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz)  # shape (bsz, start_n_top, hsz)\n",
        "            start_states = torch.gather(hidden_states, -2, start_top_index_exp)  # shape (bsz, start_n_top, hsz)\n",
        "            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1)  # shape (bsz, slen, start_n_top, hsz)\n",
        "\n",
        "            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(\n",
        "                start_states\n",
        "            )  # shape (bsz, slen, start_n_top, hsz)\n",
        "            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n",
        "            end_logits = self.end_logits(hidden_states_expanded, \n",
        "                                         start_states=start_states, \n",
        "                                         p_mask=p_mask) \n",
        "            end_probs = torch.softmax(end_logits, dim=1)  # shape (bsz, slen, start_n_top)\n",
        "\n",
        "            end_top_probs, end_top_index = torch.topk(\n",
        "                end_probs, self.top_k_end, dim=1\n",
        "            )  # shape (bsz, end_n_top, start_n_top)\n",
        "            end_top_probs = end_top_probs.view(-1, self.top_k_start * self.top_k_end)\n",
        "            end_top_index = end_top_index.view(-1, self.top_k_start * self.top_k_end)\n",
        "            \n",
        "            return start_top_probs, start_top_index, end_top_probs, end_top_index, start_logits, end_logits\n",
        "    \n",
        "def get_loss(criterion, \n",
        "             start_logits, \n",
        "             start_positions,\n",
        "             end_logits,\n",
        "             end_positions\n",
        "            ):\n",
        "    start_loss = criterion(start_logits, start_positions)\n",
        "    end_loss = criterion(end_logits, end_positions)\n",
        "    return (start_loss + end_loss) / 2\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgCYxpufb267",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d784cb34-9b64-4b8c-d381-31005b9f01fd"
      },
      "source": [
        "context = r\"\"\"\n",
        "    Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "    architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
        "    Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "    TensorFlow 2.0 and PyTorch.\n",
        "    \"\"\"\n",
        "questions = [\n",
        "    \"How many pretrained models are available in Transformers?\",\n",
        "    \"What does Transformers provide?\",\n",
        "    \"Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "\n",
        "start_positions = torch.LongTensor([95, 36, 110])\n",
        "end_positions = torch.LongTensor([97, 88, 123])\n",
        "p_mask = [[1]*12 + [0]* (125 -14) + [1,1],\n",
        "          [1]* 7 + [0]* (120 - 9) + [1,1],\n",
        "          [1]*12 + [0]* (125 -14) + [1,1],\n",
        "         ]\n",
        "\n",
        "neg_log_loss = nn.CrossEntropyLoss()\n",
        "bce_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "qa_encoder = XLNetQuestionEncoder(2, model, 768, 1, 1)\n",
        "\n",
        "optimizer = torch.optim.AdamW(encoder.parameters())\n",
        "\n",
        "encoder.train()\n",
        "optimizer.zero_grad()\n",
        "for ith, question in enumerate(questions):\n",
        "    start_logits, end_logits = qa_encoder(\n",
        "        tokenizer(question, \n",
        "                  context, \n",
        "                  add_special_tokens=True,\n",
        "                  return_tensors='pt'),\n",
        "        p_mask=torch.ByteTensor(p_mask[ith]),\n",
        "        start_positions=start_positions[ith].view(1,1,1)\n",
        "    )\n",
        "    loss = get_loss(\n",
        "        criterion,\n",
        "        start_logits, \n",
        "        start_positions[ith].view(-1),\n",
        "        end_logits,\n",
        "        end_positions[ith].view(-1)\n",
        "    )\n",
        "    print(\"\\nTrue Start: {}, True End: {}\\nMax Pred Start: {}, Max Pred End: {}\\nLoss: {}\\n\".format(\n",
        "        start_positions[ith].item(),\n",
        "        end_positions[ith].item(),\n",
        "        torch.argmax(start_logits).item(), \n",
        "        torch.argmax(end_logits).item(),\n",
        "        loss.item()\n",
        "        )\n",
        "    )\n",
        "    print(\"=\"*25)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"\\nConfirm that the gradients are computed for the original XLNet parameters.\")\n",
        "for param in encoder.parameters():\n",
        "    print(param.shape, param.grad.sum() if not param.grad is None else param.grad)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "True Start: 95, True End: 97\n",
            "Max Pred Start: 87, Max Pred End: 112\n",
            "Loss: 4.263971328735352\n",
            "\n",
            "=========================\n",
            "\n",
            "True Start: 36, True End: 88\n",
            "Max Pred Start: 100, Max Pred End: 13\n",
            "Loss: 5.032940864562988\n",
            "\n",
            "=========================\n",
            "\n",
            "True Start: 110, True End: 123\n",
            "Max Pred Start: 20, Max Pred End: 100\n",
            "Loss: 5.000000075237331e+29\n",
            "\n",
            "=========================\n",
            "\n",
            "Confirm that the gradients are computed for the original XLNet parameters.\n",
            "torch.Size([1, 1, 768]) None\n",
            "torch.Size([32000, 768]) tensor(1.3567)\n",
            "torch.Size([768, 12, 64]) tensor(0.0063)\n",
            "torch.Size([768, 12, 64]) tensor(0.0007)\n",
            "torch.Size([768, 12, 64]) tensor(-0.2502)\n",
            "torch.Size([768, 12, 64]) tensor(0.1226)\n",
            "torch.Size([768, 12, 64]) tensor(-0.5811)\n",
            "torch.Size([12, 64]) tensor(0.0150)\n",
            "torch.Size([12, 64]) tensor(-0.0031)\n",
            "torch.Size([12, 64]) tensor(0.0785)\n",
            "torch.Size([2, 12, 64]) tensor(1.4464e-09)\n",
            "torch.Size([768]) tensor(-0.0449)\n",
            "torch.Size([768]) tensor(0.0184)\n",
            "torch.Size([768]) tensor(-0.7694)\n",
            "torch.Size([768]) tensor(-1.3483)\n",
            "torch.Size([3072, 768]) tensor(-1.3852)\n",
            "torch.Size([3072]) tensor(0.0721)\n",
            "torch.Size([768, 3072]) tensor(2.1805)\n",
            "torch.Size([768]) tensor(-0.0198)\n",
            "torch.Size([768, 12, 64]) tensor(0.1485)\n",
            "torch.Size([768, 12, 64]) tensor(-1.3935)\n",
            "torch.Size([768, 12, 64]) tensor(0.6928)\n",
            "torch.Size([768, 12, 64]) tensor(0.3983)\n",
            "torch.Size([768, 12, 64]) tensor(0.8191)\n",
            "torch.Size([12, 64]) tensor(0.0192)\n",
            "torch.Size([12, 64]) tensor(0.0008)\n",
            "torch.Size([12, 64]) tensor(-0.0012)\n",
            "torch.Size([2, 12, 64]) tensor(-5.8026e-10)\n",
            "torch.Size([768]) tensor(0.1027)\n",
            "torch.Size([768]) tensor(-0.0756)\n",
            "torch.Size([768]) tensor(-0.2823)\n",
            "torch.Size([768]) tensor(0.5086)\n",
            "torch.Size([3072, 768]) tensor(2.4184)\n",
            "torch.Size([3072]) tensor(0.1850)\n",
            "torch.Size([768, 3072]) tensor(1.4611)\n",
            "torch.Size([768]) tensor(-0.0278)\n",
            "torch.Size([768, 12, 64]) tensor(0.4542)\n",
            "torch.Size([768, 12, 64]) tensor(0.3146)\n",
            "torch.Size([768, 12, 64]) tensor(-3.9306)\n",
            "torch.Size([768, 12, 64]) tensor(-0.1209)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0905)\n",
            "torch.Size([12, 64]) tensor(-0.0042)\n",
            "torch.Size([12, 64]) tensor(-0.0018)\n",
            "torch.Size([12, 64]) tensor(-0.0579)\n",
            "torch.Size([2, 12, 64]) tensor(-4.6066e-09)\n",
            "torch.Size([768]) tensor(-0.2059)\n",
            "torch.Size([768]) tensor(0.0895)\n",
            "torch.Size([768]) tensor(-0.7116)\n",
            "torch.Size([768]) tensor(-2.0367)\n",
            "torch.Size([3072, 768]) tensor(-0.9017)\n",
            "torch.Size([3072]) tensor(-0.1163)\n",
            "torch.Size([768, 3072]) tensor(-6.1434)\n",
            "torch.Size([768]) tensor(0.0721)\n",
            "torch.Size([768, 12, 64]) tensor(-0.1033)\n",
            "torch.Size([768, 12, 64]) tensor(-0.7790)\n",
            "torch.Size([768, 12, 64]) tensor(1.3858)\n",
            "torch.Size([768, 12, 64]) tensor(0.1676)\n",
            "torch.Size([768, 12, 64]) tensor(0.1088)\n",
            "torch.Size([12, 64]) tensor(-0.0094)\n",
            "torch.Size([12, 64]) tensor(-0.0017)\n",
            "torch.Size([12, 64]) tensor(-0.0063)\n",
            "torch.Size([2, 12, 64]) tensor(1.6953e-09)\n",
            "torch.Size([768]) tensor(-0.3177)\n",
            "torch.Size([768]) tensor(-0.0265)\n",
            "torch.Size([768]) tensor(-0.6462)\n",
            "torch.Size([768]) tensor(-0.0095)\n",
            "torch.Size([3072, 768]) tensor(0.6568)\n",
            "torch.Size([3072]) tensor(-0.0905)\n",
            "torch.Size([768, 3072]) tensor(-2.2850)\n",
            "torch.Size([768]) tensor(0.0386)\n",
            "torch.Size([768, 12, 64]) tensor(-1.0800)\n",
            "torch.Size([768, 12, 64]) tensor(0.0650)\n",
            "torch.Size([768, 12, 64]) tensor(4.0429)\n",
            "torch.Size([768, 12, 64]) tensor(0.2239)\n",
            "torch.Size([768, 12, 64]) tensor(0.1435)\n",
            "torch.Size([12, 64]) tensor(-0.0114)\n",
            "torch.Size([12, 64]) tensor(0.0013)\n",
            "torch.Size([12, 64]) tensor(-0.0506)\n",
            "torch.Size([2, 12, 64]) tensor(-2.9268e-08)\n",
            "torch.Size([768]) tensor(0.0102)\n",
            "torch.Size([768]) tensor(-0.0043)\n",
            "torch.Size([768]) tensor(-0.1082)\n",
            "torch.Size([768]) tensor(-0.0758)\n",
            "torch.Size([3072, 768]) tensor(2.6073)\n",
            "torch.Size([3072]) tensor(-0.1433)\n",
            "torch.Size([768, 3072]) tensor(-6.4323)\n",
            "torch.Size([768]) tensor(0.0875)\n",
            "torch.Size([768, 12, 64]) tensor(-0.1260)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0226)\n",
            "torch.Size([768, 12, 64]) tensor(-7.3731)\n",
            "torch.Size([768, 12, 64]) tensor(-0.2054)\n",
            "torch.Size([768, 12, 64]) tensor(1.2019)\n",
            "torch.Size([12, 64]) tensor(0.0229)\n",
            "torch.Size([12, 64]) tensor(-0.0008)\n",
            "torch.Size([12, 64]) tensor(-0.0140)\n",
            "torch.Size([2, 12, 64]) tensor(-1.3875e-08)\n",
            "torch.Size([768]) tensor(-0.3383)\n",
            "torch.Size([768]) tensor(0.0752)\n",
            "torch.Size([768]) tensor(-0.0038)\n",
            "torch.Size([768]) tensor(-0.2227)\n",
            "torch.Size([3072, 768]) tensor(11.6210)\n",
            "torch.Size([3072]) tensor(-0.6505)\n",
            "torch.Size([768, 3072]) tensor(6.2449)\n",
            "torch.Size([768]) tensor(-0.0987)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0337)\n",
            "torch.Size([768, 12, 64]) tensor(0.0015)\n",
            "torch.Size([768, 12, 64]) tensor(-3.3932)\n",
            "torch.Size([768, 12, 64]) tensor(0.3783)\n",
            "torch.Size([768, 12, 64]) tensor(3.2205)\n",
            "torch.Size([12, 64]) tensor(0.0065)\n",
            "torch.Size([12, 64]) tensor(-0.0018)\n",
            "torch.Size([12, 64]) tensor(-0.0036)\n",
            "torch.Size([2, 12, 64]) tensor(1.3893e-08)\n",
            "torch.Size([768]) tensor(0.2274)\n",
            "torch.Size([768]) tensor(0.1322)\n",
            "torch.Size([768]) tensor(-0.0291)\n",
            "torch.Size([768]) tensor(-0.7393)\n",
            "torch.Size([3072, 768]) tensor(0.6735)\n",
            "torch.Size([3072]) tensor(-0.0474)\n",
            "torch.Size([768, 3072]) tensor(0.6900)\n",
            "torch.Size([768]) tensor(-0.0014)\n",
            "torch.Size([768, 12, 64]) tensor(-0.3758)\n",
            "torch.Size([768, 12, 64]) tensor(0.0291)\n",
            "torch.Size([768, 12, 64]) tensor(-6.6945)\n",
            "torch.Size([768, 12, 64]) tensor(-3.2813)\n",
            "torch.Size([768, 12, 64]) tensor(-4.4186)\n",
            "torch.Size([12, 64]) tensor(-0.0062)\n",
            "torch.Size([12, 64]) tensor(-0.0009)\n",
            "torch.Size([12, 64]) tensor(0.0206)\n",
            "torch.Size([2, 12, 64]) tensor(3.1177e-09)\n",
            "torch.Size([768]) tensor(-0.2594)\n",
            "torch.Size([768]) tensor(0.1830)\n",
            "torch.Size([768]) tensor(-0.0056)\n",
            "torch.Size([768]) tensor(0.1887)\n",
            "torch.Size([3072, 768]) tensor(12.5694)\n",
            "torch.Size([3072]) tensor(-0.6247)\n",
            "torch.Size([768, 3072]) tensor(3.1289)\n",
            "torch.Size([768]) tensor(-0.0793)\n",
            "torch.Size([768, 12, 64]) tensor(0.1730)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0063)\n",
            "torch.Size([768, 12, 64]) tensor(-19.0155)\n",
            "torch.Size([768, 12, 64]) tensor(0.1023)\n",
            "torch.Size([768, 12, 64]) tensor(-1.5656)\n",
            "torch.Size([12, 64]) tensor(0.0039)\n",
            "torch.Size([12, 64]) tensor(-0.0012)\n",
            "torch.Size([12, 64]) tensor(-0.0079)\n",
            "torch.Size([2, 12, 64]) tensor(-2.6066e-08)\n",
            "torch.Size([768]) tensor(-0.0123)\n",
            "torch.Size([768]) tensor(-0.2425)\n",
            "torch.Size([768]) tensor(0.0416)\n",
            "torch.Size([768]) tensor(-0.3892)\n",
            "torch.Size([3072, 768]) tensor(-0.7567)\n",
            "torch.Size([3072]) tensor(0.0800)\n",
            "torch.Size([768, 3072]) tensor(1.2434)\n",
            "torch.Size([768]) tensor(-0.0089)\n",
            "torch.Size([768, 12, 64]) tensor(0.5666)\n",
            "torch.Size([768, 12, 64]) tensor(0.0307)\n",
            "torch.Size([768, 12, 64]) tensor(5.3658)\n",
            "torch.Size([768, 12, 64]) tensor(0.1935)\n",
            "torch.Size([768, 12, 64]) tensor(-2.1732)\n",
            "torch.Size([12, 64]) tensor(-0.0402)\n",
            "torch.Size([12, 64]) tensor(1.7912e-05)\n",
            "torch.Size([12, 64]) tensor(0.0183)\n",
            "torch.Size([2, 12, 64]) tensor(1.0234e-08)\n",
            "torch.Size([768]) tensor(0.1130)\n",
            "torch.Size([768]) tensor(-0.1742)\n",
            "torch.Size([768]) tensor(0.1034)\n",
            "torch.Size([768]) tensor(0.8056)\n",
            "torch.Size([3072, 768]) tensor(-0.5503)\n",
            "torch.Size([3072]) tensor(0.1784)\n",
            "torch.Size([768, 3072]) tensor(-1.5986)\n",
            "torch.Size([768]) tensor(-0.1443)\n",
            "torch.Size([768, 12, 64]) tensor(2.7333)\n",
            "torch.Size([768, 12, 64]) tensor(-0.0081)\n",
            "torch.Size([768, 12, 64]) tensor(40.1856)\n",
            "torch.Size([768, 12, 64]) tensor(-2.2006)\n",
            "torch.Size([768, 12, 64]) tensor(6.6083)\n",
            "torch.Size([12, 64]) tensor(-0.0135)\n",
            "torch.Size([12, 64]) tensor(-0.0008)\n",
            "torch.Size([12, 64]) tensor(-0.0283)\n",
            "torch.Size([2, 12, 64]) tensor(2.8738e-08)\n",
            "torch.Size([768]) tensor(0.6568)\n",
            "torch.Size([768]) tensor(-1.6965)\n",
            "torch.Size([768]) tensor(-0.5415)\n",
            "torch.Size([768]) tensor(-0.6481)\n",
            "torch.Size([3072, 768]) tensor(-6.1584)\n",
            "torch.Size([3072]) tensor(0.7130)\n",
            "torch.Size([768, 3072]) tensor(-2.4611)\n",
            "torch.Size([768]) tensor(0.0378)\n",
            "torch.Size([768, 12, 64]) tensor(3.2718)\n",
            "torch.Size([768, 12, 64]) tensor(0.0086)\n",
            "torch.Size([768, 12, 64]) tensor(28.6241)\n",
            "torch.Size([768, 12, 64]) tensor(0.9142)\n",
            "torch.Size([768, 12, 64]) tensor(-5.5994)\n",
            "torch.Size([12, 64]) tensor(-0.0003)\n",
            "torch.Size([12, 64]) tensor(-0.0026)\n",
            "torch.Size([12, 64]) tensor(-0.0361)\n",
            "torch.Size([2, 12, 64]) tensor(3.7834e-07)\n",
            "torch.Size([768]) tensor(-1.2772)\n",
            "torch.Size([768]) tensor(3.5014)\n",
            "torch.Size([768]) tensor(0.5100)\n",
            "torch.Size([768]) tensor(-0.0341)\n",
            "torch.Size([3072, 768]) tensor(65.6120)\n",
            "torch.Size([3072]) tensor(-1.2471)\n",
            "torch.Size([768, 3072]) tensor(130.3757)\n",
            "torch.Size([768]) tensor(0.4395)\n",
            "torch.Size([768, 768]) tensor(0.)\n",
            "torch.Size([768]) tensor(0.)\n",
            "torch.Size([2, 768]) tensor(0.)\n",
            "torch.Size([2]) tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXzlKiGKb269",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "56389516-5358-411a-dc18-50fb774a81da"
      },
      "source": [
        "# inference\n",
        "context = r\"\"\"\n",
        "    Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "    architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
        "    Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "    TensorFlow 2.0 and PyTorch.\n",
        "    \"\"\"\n",
        "questions = [\n",
        "    \"How many pretrained models are available in Transformers?\",\n",
        "    \"What does Transformers provide?\",\n",
        "    \"Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "qa_encoder.eval()\n",
        "for ith, question in enumerate(questions):\n",
        "    inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "    \n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    start_probs, start_index, end_probs, end_index, stt_logits, end_logits = qa_encoder(\n",
        "        inputs, \n",
        "        p_mask=torch.ByteTensor(p_mask[ith])\n",
        "    )\n",
        "    max_start_prob = torch.max(start_probs).item()\n",
        "    max_end_prob = torch.max(end_probs).item()\n",
        "    start = start_index[torch.argmax(start_probs)].item()\n",
        "    end = end_index[torch.argmax(end_probs)].item()\n",
        "    \n",
        "#     print(len(input_ids), stt_logits.shape, end_logits.shape)\n",
        "#     print(tokenizer.convert_ids_to_tokens(input_ids).index('?'))\n",
        "    print(\"=\"*25)\n",
        "    print(\"True start: {}, True end: {}\".format(\n",
        "        start_positions[ith].item(),\n",
        "        end_positions[ith].item()\n",
        "        ))\n",
        "    print(\"start prob: {:0.4f}, start idx: {}, end prob: {:0.4f}, end idx: {}\".format(\n",
        "        max_start_prob,\n",
        "        start,\n",
        "        max_end_prob,\n",
        "        end,\n",
        "    ))\n",
        "    print(\"-\"*25)\n",
        "    print(\"Question: '{}'\".format(question))\n",
        "    print(\"Answer: '{}'\".format(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[start:end]))))\n",
        "    print(\"=\"*25)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================\n",
            "True start: 95, True end: 97\n",
            "start prob: 0.0090, start idx: 42, end prob: 0.0090, end idx: 49\n",
            "-------------------------\n",
            "Question: 'How many pretrained models are available in Transformers?'\n",
            "Answer: '-purpose architectures (BER'\n",
            "=========================\n",
            "=========================\n",
            "True start: 36, True end: 88\n",
            "start prob: 0.0090, start idx: 39, end prob: 0.0090, end idx: 89\n",
            "-------------------------\n",
            "Question: 'What does Transformers provide?'\n",
            "Answer: 'architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with'\n",
            "=========================\n",
            "=========================\n",
            "True start: 110, True end: 123\n",
            "start prob: 0.0090, start idx: 40, end prob: 0.0090, end idx: 12\n",
            "-------------------------\n",
            "Question: 'Transformers provides interoperability between which frameworks?'\n",
            "Answer: ''\n",
            "=========================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwYZU9c4b26-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}