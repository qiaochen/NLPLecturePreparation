{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivations of EM steps for Mixture of Beta Binomial Distribution\n",
    "\n",
    "- Beta-Binomial Distribution (BB): \n",
    "$$\n",
    "\\begin{align}\n",
    "p(y^{(i)} | \\theta) &= \\int_{0}^{(1)}Bin(y^i|n,p) \\cdot Beta(p|\\alpha, \\beta) dp \\\\\n",
    "                &= {{n}\\choose{y^{(i)}}}\\frac{1}{B(\\alpha,\\beta)} \\int_{0}^{1} p^{y^{(i)}+ \\alpha -1} (1-p)^{n - y^{(i)} + \\beta - 1} dp   \\\\\n",
    "                &= {{n}\\choose{y^{(i)}}}\\frac{B(y^{(i)}+\\alpha, n-y^{(i)}+\\beta)}{B(\\alpha, \\beta)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Mixture of Beta-Binomial Distribution (MBB), joint probablility, suppose there are K components, and let the variables be $\\mathbf{\\gamma} = [\\gamma_{0}, \\gamma_{1}, ..., \\gamma_{k}, \\gamma_{K}]$, $\\gamma_{k} \\text{~} Bin(\\gamma_{k}|\\pi_k)$:\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y^{(i)}, \\gamma^{(i)} | \\theta, \\pi) &= \\prod_{k=1}^{K} p(y^{(i)}, \\gamma_{k}^{(i)}| \\theta_{k}, \\pi_k)^{\\gamma^{(i)}_k} \\\\\n",
    "                                                      &= \\prod_{k=1}^{K} \\{p(y^{(i)} | \\theta_{k}) \\pi_k \\}^{\\gamma_k^{(i)}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Log likelihood of the full data (MBB):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(y^{(i)}, \\gamma^{(i)} | \\theta, \\pi) &= \\sum_{k=1}^{K} \\bigg \\{ \\gamma^{(i)}_k \\big(\\log\\pi_k + \\log p(y^{(i)} | \\theta_{k}) \\big)\n",
    "\\bigg\\} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E step:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E_{\\gamma^{(i)}_k \\text{~} p(\\pi_k|y^{(i)})}[\\log p(y^{(i)}, \\gamma^{(i)}] &= E\\bigg[\\sum_{k=1}^{K} \\big \\{ \\gamma^{(i)}_k \\big(\\log\\pi_k + \\log p(y^{(i)} | \\theta_k) \\big)\\big\\} \\bigg] \\\\\n",
    "                      &= \\sum_{k=1}^{K} \\big \\{ E[\\gamma^{(i)}_k] \\big(\\log\\pi_k + \\log p(y^{(i)} | \\theta_k) \\big)\\big\\} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "As:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bar{\\gamma}_k^{(i)} = E(\\gamma^{(i)}_k|y^{(i)}, \\theta, \\pi) &= p(\\gamma_k = 1| y^{(i)}, \\theta, \\pi)   \\text(- Expection of Bernoulli distribution)\\\\\n",
    "                                       &= \\frac{p(\\gamma^{(i)}_k=1, y^{(i)}| \\theta, \\pi)}{\\sum_{k=1}^{K}p(\\gamma_k^{(i)}=1, y^{(i)}| \\theta, \\pi)} \\\\\n",
    "                                       &= \\frac{p(y^{(i)}|\\gamma^{(i)}_k=1, \\theta, \\pi) \\cdot p(\\gamma^{(i)}_k=1|\\pi)}{\\sum_{k=1}^{K}p(y^{(i)}|\\gamma^{(i)}_k=1, \\theta, \\pi) \\cdot p(\\gamma^{(i)}_k=1|\\pi)} \\\\\n",
    "                                       &= \\frac{ {{n}\\choose{y^{(i)}}} \\frac{B(y^{(i)} + \\alpha_k, n-y^{(i)} + \\beta_k)}{B(\\alpha_k, \\beta_k)} \\cdot \\pi_k}{ \\sum_{k=1}^{K}{{n}\\choose{y^{(i)}}} \\frac{B(y^{(i)} + \\alpha_k, n-y^{(i)} + \\beta_k)}{B(\\alpha_k, \\beta_k)} \\cdot \\pi_k} \\\\\n",
    "                                       &= \\frac{\\frac{B(y^{(i)} + \\alpha_k, n-y^{(i)} + \\beta_k)}{B(\\alpha_k, \\beta_k)} \\cdot \\pi_k}{ \\sum_{k=1}^{K} \\frac{B(y^{(i)} + \\alpha_k, n-y^{(i)} + \\beta_k)}{B(\\alpha_k, \\beta_k)} \\cdot \\pi_k}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## M step\n",
    "M-step hence is to optimize:\n",
    "\n",
    "$$\n",
    " \\max_{\\pi, \\theta}\\bigg\\{\\sum_{k=1}^{K} \\big \\{ \\bar{\\gamma}_k^{(i)} \\big(\\log\\pi_k + \\log p(y^{(i)} | \\theta_k) \\big)\\big\\} \\bigg\\} \\\\\n",
    " = \\max_{\\pi, \\theta}\\bigg\\{ \\sum_{k=1}^{K} \\big \\{ \\bar{\\gamma}_k^{(i)} \\big(\\log\\pi_k + \\log{{n}\\choose{y^{(i)}}} + \\log B(y^{(i)}+\\alpha_k, n-y^{(i)}+\\beta_k) - \\log{B(\\alpha_k, \\beta_k)} \\big)\\big\\} \\bigg\\} \\\\\n",
    "s.t. \\sum_{k=1}^{K} \\pi_k = 1, \\\\ \\pi_k \\geq 0, k=1,...,K\\\\ \\alpha > 0, \\\\ \\beta > 0\n",
    "$$\n",
    "\n",
    "\n",
    "Using all the data points:\n",
    "\n",
    "$$\n",
    "\\max_{\\pi, \\theta}\\bigg\\{ \\sum_{i=1}^{N}\\sum_{k=1}^{K} \\big \\{ \\bar{\\gamma}_k^{(i)} \\big(\\log\\pi_k + \\log{{n}\\choose{y^{(i)}}} + \\log B(y^{(i)}+\\alpha_k, n-y^{(i)}+\\beta_k) - \\log{B(\\alpha_k, \\beta_k)} \\big)\\big\\} \\bigg\\} \\\\\n",
    "s.t. \\sum_{k=1}^{K} \\pi_k = 1 \\\\ \\pi_k \\geq 0, k=1,...,K \\\\ \\alpha > 0, \\\\ \\beta > 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import beta, comb, gammaln\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import betabinom, bernoulli\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureBetaBinomial:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_components=2,\n",
    "                 max_m_step_iter=250,\n",
    "                 tor=1e-12\n",
    "                ):\n",
    "        super(MixtureBetaBinomial, self).__init__()\n",
    "        self.n_components = n_components\n",
    "        self.max_m_step_iter = max_m_step_iter\n",
    "        self.tor = tor\n",
    "        self.params = None\n",
    "        \n",
    "    def _E_step(self, data, a, b, pi):\n",
    "        y, n = data\n",
    "        log_E_gamma_k = np.log(pi) + gammaln(y+a) + gammaln(n-y+b) + gammaln(a+b) - \\\n",
    "                (gammaln(a) + gammaln(b) + gammaln(n+a+b))\n",
    "        return np.exp(log_E_gamma_k)\n",
    "    \n",
    "    def _get_constraints(self):\n",
    "        constraints = ({'type':'eq', 'fun': lambda params: np.sum(params[2*self.n_components:]) - 1})\n",
    "        return constraints\n",
    "    \n",
    "    def _get_bounds(self):\n",
    "        bounds = [(0, None)]* (3*self.n_components)\n",
    "        return tuple(bounds)\n",
    "    \n",
    "    def _get_nnl_fun(self):\n",
    "        def neg_log_likelihood(params, *args):\n",
    "            \"\"\"\n",
    "            params: 1-D arrays: [a_0, a_1, a_2, ..., b_0, b_1, b_2, ..., pi_0, pi_1, pi_2,...]\n",
    "            args: list of function arguments, \n",
    "                args[0] is the data: tuple of list (y, n)\n",
    "                args[1] is \\hat{\\gamma}, expectation of the latent variables\n",
    "                args[2] is number of components, int\n",
    "            \"\"\"\n",
    "            y, n = args[0]\n",
    "            E_gammas = args[1]\n",
    "            n_components = args[2]\n",
    "\n",
    "        #     log_likelihood = 0\n",
    "        #     for i in range(n_components):\n",
    "        #         a, b, pi = params[i], params[i+n_components], params[i+2*n_components]\n",
    "        #         E_gamma_k = E_gammas[i]\n",
    "        #         log_pdf =  E_gamma_k * (np.log(pi) + np.log(comb(n, y)) + np.log(beta(y + a, n - y + b)) - np.log(beta(a, b)))\n",
    "        #         log_likelihood += np.sum(log_pdf)\n",
    "\n",
    "            # numerical stabler computation when n and y are very large\n",
    "            # https://en.wikipedia.org/wiki/Beta-binomial_distribution#Beta-binomial_distribution_as_a_compound_distribution\n",
    "            log_likelihood = 0\n",
    "            for i in range(n_components):\n",
    "                a, b, pi = params[i], params[i+n_components], params[i+2*n_components]\n",
    "                E_gamma_k = E_gammas[i]\n",
    "                log_pdf = E_gamma_k * (np.log(pi) + gammaln(n+1) + gammaln(y+a) + gammaln(n-y+b) + gammaln(a+b) - \\\n",
    "                 (gammaln(y+1) + gammaln(n-y+1) + gammaln(a) + gammaln(b) + gammaln(n+a+b)))\n",
    "                log_likelihood += np.sum(log_pdf)\n",
    "            return -log_likelihood\n",
    "        \n",
    "        return neg_log_likelihood\n",
    "    \n",
    "    def _M_step(self, data, E_gammas, params):\n",
    "        constraints = self._get_constraints()\n",
    "        bounds = self._get_bounds()\n",
    "        nll_fun = self._get_nnl_fun()\n",
    "        res = minimize(nll_fun, x0=params,\n",
    "                args=(data, E_gammas, self.n_components),\n",
    "                bounds=bounds,\n",
    "                method='SLSQP',\n",
    "                options={'disp': True, 'maxiter': self.max_m_step_iter},\n",
    "                constraints=constraints #  for solvers: COBYLA, SLSQP and trust-constr\n",
    "                )\n",
    "        return res\n",
    "    \n",
    "    def _perturbate(self, array):\n",
    "        array += np.random.uniform(-0.025, 0.025, np.shape(array))\n",
    "        while np.any(array) < 0 or np.any(array) > 1:\n",
    "            array += np.random.uniform(-0.025, 0.025, np.shape(array))\n",
    "        return array\n",
    "        \n",
    "    \n",
    "    def _init_gamma_with_kmeans(self, data):\n",
    "        # set all alpha and beta to be 0.5\n",
    "        y, n = data\n",
    "        sample_size = len(y)\n",
    "        proportions = y / n\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=self.n_components,\n",
    "            max_iter=500,\n",
    "            random_state=44\n",
    "           )\n",
    "        kmeans.fit(proportions.reshape(-1,1))\n",
    "        gammars = np.zeros((sample_size, self.n_components), dtype=np.float) \n",
    "        gammars[range(sample_size), kmeans.labels_] = 1.0\n",
    "        \n",
    "        pi = np.sum(gammars, axis=0)\n",
    "        pi = pi / np.sum(pi)\n",
    "        \n",
    "        params = np.concatenate([np.ones(self.n_components, dtype=np.float)*0.9, \n",
    "                                 np.ones(self.n_components, dtype=np.float)*0.9,\n",
    "                                 pi])\n",
    "        \n",
    "        return self._perturbate(gammars.T), params\n",
    "        \n",
    "    \n",
    "    def EM(self, data, max_iters=250):\n",
    "        \"\"\"\n",
    "        Data: tuple of lists: (y, n)\n",
    "        \"\"\"\n",
    "        # init parameters 1) k-means for latent gammas, \n",
    "        #                 2) MLE for pis given gammas, (take proportion)\n",
    "        #                 3) MLE for alphas and betas given pis and noised gammas values\n",
    "        E_gammas, params = self._init_gamma_with_kmeans(data)\n",
    "        init_res = self._M_step(data, E_gammas, params) # init\n",
    "        params = init_res.x\n",
    "        print(\"=\"*25)\n",
    "        print(\"Init params: {}\".format(params))\n",
    "        print(\"=\"*25)\n",
    "        \n",
    "        neg_log_likelihood = self._get_nnl_fun()        \n",
    "        losses = [sys.maxsize]\n",
    "        for ith in range(max_iters):\n",
    "            \n",
    "            # E step\n",
    "            for i in range(self.n_components):\n",
    "                a, b, pi = params[i], params[i+self.n_components], params[i+2*self.n_components]\n",
    "                E_gammas[i] = self._E_step(data, a, b, pi)\n",
    "\n",
    "            # normalize as they are havn't been\n",
    "            E_gammas = E_gammas / np.sum(E_gammas, axis=0)\n",
    "            \n",
    "            # M step\n",
    "            res = self._M_step(data, E_gammas, params)\n",
    "            params = res.x\n",
    "            \n",
    "            # current NLL loss\n",
    "            losses.append(neg_log_likelihood(params, data, E_gammas, self.n_components))\n",
    "            print(\"=\"*10, \"Iteration {}\".format(ith+1), \"=\"*10)\n",
    "            print(\"Negative LogLikelihood Loss: {}\".format(losses[-1]))\n",
    "            print(\"=\"*25)\n",
    "            \n",
    "            improvement = losses[-2] - losses[-1]\n",
    "            if improvement < self.tor:\n",
    "                print(\"Improvement halts, early stop training.\")\n",
    "                break            \n",
    "            \n",
    "        self.params = params\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## simulation experiment\n",
    "# sample gammars\n",
    "n_samples = 2000\n",
    "n_trials = 1000\n",
    "pis = [0.6, 0.4]\n",
    "alphas, betas = [2, 0.9], [0.1, 5]\n",
    "\n",
    "gammars = bernoulli.rvs(pis[0], size=n_samples)\n",
    "n_pos_events = sum(gammars)\n",
    "n_neg_events = n_samples - n_pos_events\n",
    "\n",
    "ys_of_type1 = betabinom.rvs(n_trials, alphas[0], betas[0], size=n_pos_events)\n",
    "ys_of_type2 = betabinom.rvs(n_trials, alphas[1], betas[1], size=n_neg_events)\n",
    "\n",
    "\n",
    "ys = np.concatenate((ys_of_type1, ys_of_type2))\n",
    "ns = np.ones(n_samples, dtype=np.int) * n_trials\n",
    "len(ys), len(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in subtract\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:90: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: RuntimeWarning: invalid value encountered in add\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 9860.294035431058\n",
      "            Iterations: 31\n",
      "            Function evaluations: 283\n",
      "            Gradient evaluations: 31\n",
      "=========================\n",
      "Init params: [2.81779529 0.90954636 0.10852537 5.32634319 0.59492329 0.40507671]\n",
      "=========================\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 9901.521095932787\n",
      "            Iterations: 16\n",
      "            Function evaluations: 160\n",
      "            Gradient evaluations: 16\n",
      "========== Iteration 1 ==========\n",
      "Negative LogLikelihood Loss: 9901.521095932787\n",
      "=========================\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 9912.409511748694\n",
      "            Iterations: 8\n",
      "            Function evaluations: 88\n",
      "            Gradient evaluations: 8\n",
      "========== Iteration 2 ==========\n",
      "Negative LogLikelihood Loss: 9912.409511748694\n",
      "=========================\n",
      "Improvement halts, early stop training.\n",
      "[2.40088949 0.86540789 0.10580962 4.95823576 0.59897059 0.40102941]\n",
      "[2, 0.9] [0.1, 5] [0.6, 0.4]\n"
     ]
    }
   ],
   "source": [
    "em_mbb = MixtureBetaBinomial(\n",
    "     n_components=2,\n",
    "     max_m_step_iter=250,\n",
    "     tor=1e-20)\n",
    "params = em_mbb.EM((ys, ns))\n",
    "print(params)\n",
    "print(alphas, betas, pis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
